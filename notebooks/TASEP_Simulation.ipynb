{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating TASEP Models\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MicroLive Notebook\n",
    "==================\n",
    "This notebook requires MicroLive to be installed:\n",
    "    pip install tasep_models\n",
    "\n",
    "For development mode:\n",
    "    pip install -e /path/to/tasep_models\n",
    "\"\"\"\n",
    "# MicroLive imports\n",
    "# from microlive import microscopy as mi\n",
    "# from microlive.utils.device import check_gpu_status\n",
    "\n",
    "# Verify GPU support\n",
    "check_gpu_status()\n",
    "\n",
    "# Standard scientific imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and plotting the file sequences.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "import tasep_models as tm\n",
    "from tasep_models import *\n",
    "#tag_dict = {'GFP': GFP_TAG, 'HA': HA_TAG, 'U': U_TAG, 'SUN': SUN_TAG, 'ALFA': ALFA_TAG}\n",
    "tag_file = 'UTAG'\n",
    "if tag_file == 'SUN':\n",
    "    tag_sequence = tag_dict['SUN']\n",
    "    dna_file_path = pathlib.Path( '/Users/nzlab-la/Desktop/micro/gene_sequences/utag_project/pNZ266(pUB-24xGCN4-KDM5B-MS2).dna' )             # SunTag\n",
    "elif tag_file == 'UTAG':\n",
    "    tag_sequence = tag_dict['U']\n",
    "    dna_file_path = pathlib.Path( '/Users/nzlab-la/Desktop/micro/gene_sequences/utag_project/pNZ208(pUB-24xUTagFullLength-KDM5B-MS2).dna' )   # UTAG\n",
    "elif tag_file == 'ALFA':\n",
    "    tag_sequence = tag_dict['ALFA']\n",
    "    dna_file_path = pathlib.Path( '/Users/nzlab-la/Desktop/micro/gene_sequences/utag_project/pNZ267 (pUB-24xALFAtag-KDM5B-MS2).dna' )         # AlfaTag\n",
    "\n",
    "dna_file_path.name.split('.')[0]\n",
    "plasmid_name = dna_file_path.name.split('.')[0].replace('(','_').replace(')','_')\n",
    "plasmid_name\n",
    "\n",
    "# Creating a results folder for outputs\n",
    "results_folder = current_dir.joinpath('results_simulations') #/ plasmid_name\n",
    "results_folder.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "#dna_file_path = pathlib.Path( '/Users/nzlab-la/Desktop/micro/gene_sequences/utag_project/pNZ208(pUB-24xUTagFullLength-KDM5B-MS2).dna' )   # UTAG\n",
    "#dna_file_path = pathlib.Path( '/Users/nzlab-la/Desktop/micro/gene_sequences/utag_project/pNZ267 (pUB-24xALFAtag-KDM5B-MS2).dna' )         # AlfaTag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the sequence and extracting the elongation rates\n",
    "protein, rna, dna, indexes_tags, indexes_pauses, seq_record, graphic_features  = read_sequence(seq=dna_file_path, min_protein_length=50,TAG=[tag_sequence])\n",
    "plasmid_figure = plot_plasmid(seq_record, graphic_features,figure_width=25, figure_height=3)\n",
    "\n",
    "gene_length = len(protein)+1 # adding 1 to account for the stop codon\n",
    "tag_positions_first_probe_vector = indexes_tags[0]\n",
    "tag_positions_second_probe_vector = indexes_tags[1] if len(indexes_tags) > 1 else None\n",
    "\n",
    "first_probe_position_vector = create_probe_vector(tag_positions_first_probe_vector, gene_length)\n",
    "second_probe_position_vector = create_probe_vector(tag_positions_second_probe_vector, gene_length) if tag_positions_second_probe_vector is not None else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation parameters \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initial conditions\n",
    "ki = 0.03  # Initiation rate\n",
    "global_elongation_rate = 5 #3.7  # Elongation rates for positions 1 to N-1\n",
    "number_repetitions = 200\n",
    "burnin_time = 2500\n",
    "t_max = 360*10 #timePerturbationApplication + 25*60  # Maximum time\n",
    "step_size_in_sec = 5 # 5\n",
    "time_array = np.arange(0, t_max, step_size_in_sec)\n",
    "number_tested_parameters = 5\n",
    "downsample = False\n",
    "downsample_factor = 3\n",
    "MAD_THRESHOLD_FACTOR = 4\n",
    "efficiency_list = [1]  # binding efficiency for the two probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ke = calculate_codon_elongation_rates (rna, global_elongation_rate=global_elongation_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_vector_first_signal_ode,intensity_vector_second_signal_ode = simulate_TASEP_ODE(ki, ke, gene_length, t_max,first_probe_position_vector,second_probe_position_vector,burnin_time, time_interval_in_seconds= step_size_in_sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_probe_position_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_elongation_rate = None # this is a signal to the SSA to use the elongation rates considering sequence variability.\n",
    "list_ribosome_trajectories,list_occupancy_output, matrix_intensity_first_signal_RT, matrix_intensity_second_signal_RT = simulate_TASEP_SSA(ki, ke, gene_length, t_max,\n",
    "                                time_interval_in_seconds=step_size_in_sec,\n",
    "                                number_repetitions=number_repetitions, \n",
    "                                first_probe_position_vector=first_probe_position_vector, \n",
    "                                second_probe_position_vector=second_probe_position_vector,\n",
    "                                burnin_time=burnin_time,\n",
    "                                constant_elongation_rate=constant_elongation_rate,\n",
    "                                efficiency_list= efficiency_list,\n",
    "                                fast_output=False)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the mean occupancy across all frames\n",
    "ribosomal_density = np.round( (gene_length/global_elongation_rate) *ki , 1)\n",
    "print(f'Ribosomal density: {ribosomal_density} ribosomes per gene length ({gene_length} codons)')\n",
    "ribosomal_footprint = 10\n",
    "# ribosomal density covering the RNA length\n",
    "ribosomal_density_coverage = np.round( (ribosomal_density * ribosomal_footprint) / gene_length, 2)\n",
    "percentage_coverage = (ribosomal_density_coverage * 100)  # percentage coverage of the gene length by ribosomes\n",
    "print(f'Ribosomal density coverage: {np.round(ribosomal_density_coverage,2)} ribosomes per gene length ({gene_length} codons, {percentage_coverage:.2f}% coverage)')\n",
    "#print(f'Ribosomal density coverage: {ribosomal_density_coverage} ribosomes per gene length ({gene_length} codons)')\n",
    "len(list_occupancy_output)\n",
    "# each element in the list in list_occupancy_output is an array with shape [gene_location, frame]. Calculate the number of non-zero elements in each column, and then take the mean across the frames.\n",
    "occupancy_array = np.array([np.mean(np.count_nonzero(occupancy, axis=0)) for occupancy in list_occupancy_output])\n",
    "\n",
    "# print validated ribosomal density with simulated data\n",
    "print(f'Ribosomal density with simulated data: { np.round( np.mean(occupancy_array), 2)} ribosomes per gene length ({gene_length} codons)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gene_length * ki) /global_elongation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((20*10) / 1970 )*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_occupancy_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and std of the matrix_intensity_first_signal_RT and matrix_intensity_second_signal_RT\n",
    "mean_first_signal_RT = np.mean(matrix_intensity_first_signal_RT, axis=0)\n",
    "sem_first_signal_RT = np.std(matrix_intensity_first_signal_RT, axis=0)/np.sqrt(number_repetitions)\n",
    "if second_probe_position_vector is not None:\n",
    "    mean_second_signal_RT = np.mean(matrix_intensity_second_signal_RT, axis=0)\n",
    "    sem_second_signal_RT = np.std(matrix_intensity_second_signal_RT, axis=0)/np.sqrt(number_repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(matrix_intensity_first_signal_RT, intensity_vector_first_signal_ode, time_array, number_repetitions, plot_color = 'orangered'):\n",
    "    # --- Set fonts and background as before ---\n",
    "    plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "    plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "    plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "    plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "    plt.rcParams[\"axes.labelcolor\"] = \"black\"\n",
    "    plt.rcParams[\"xtick.color\"] = \"black\"\n",
    "    plt.rcParams[\"ytick.color\"] = \"black\"\n",
    "\n",
    "    # --- Determine the global intensity range from both datasets ---\n",
    "    global_min = min(matrix_intensity_first_signal_RT.min(), intensity_vector_first_signal_ode.min())\n",
    "    global_max = max(matrix_intensity_first_signal_RT.max(), intensity_vector_first_signal_ode.max())\n",
    "\n",
    "    # --- Create subplots: left for trajectories, right for histogram ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 3), gridspec_kw={'width_ratios': [4, 1]})\n",
    "\n",
    "    # --- Left Plot: Trajectories ---\n",
    "    for i in range(number_repetitions):\n",
    "        if i == 0:\n",
    "            ax1.plot(time_array, matrix_intensity_first_signal_RT[i, :],\n",
    "                    label='SSA', color=plot_color, alpha=1, linewidth=2)\n",
    "        else:\n",
    "            ax1.plot(time_array, matrix_intensity_first_signal_RT[i, :],\n",
    "                    color=plot_color, alpha=0.1, linewidth=0.4)\n",
    "    ax1.plot(time_array, intensity_vector_first_signal_ode, label='ODE', color='k', linewidth=3)\n",
    "\n",
    "    ax1.set_xlabel('Time (s)', fontsize=20)\n",
    "    ax1.set_ylabel('Intensity (a.u.)', fontsize=20)\n",
    "    ax1.set_ylim(global_min, global_max)\n",
    "\n",
    "    # Set the axes frame with a distinct black border for ax1:\n",
    "    for spine in ax1.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(2)\n",
    "        spine.set_color('black')\n",
    "\n",
    "    # Place the legend in the upper right corner with a black border\n",
    "    legend1 = ax1.legend(loc='upper right', fontsize=14)\n",
    "    legend1.get_frame().set_edgecolor('black')\n",
    "    legend1.get_frame().set_linewidth(1.5)\n",
    "\n",
    "    ax1.grid(False)  # Remove grid lines\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "\n",
    "    # --- Right Plot: Horizontal Histogram of SSA Trajectories ---\n",
    "    # Flatten all SSA trajectory values into a single array\n",
    "    ssa_values = matrix_intensity_first_signal_RT.flatten()\n",
    "\n",
    "    ax2.hist(ssa_values, bins=100, orientation='horizontal',\n",
    "            color=plot_color, alpha=0.7)\n",
    "    ax2.set_xlabel('Counts', fontsize=20)\n",
    "    ax2.set_ylabel('Intensity (a.u.)', fontsize=20)\n",
    "    ax2.set_ylim(global_min, global_max)\n",
    "    # set axis font size\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    # Set the axes frame with a distinct black border for ax2:\n",
    "    for spine in ax2.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(2)\n",
    "        spine.set_color('black')\n",
    "\n",
    "    ax2.grid(False)  # Remove grid lines\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories(matrix_intensity_first_signal_RT, intensity_vector_first_signal_ode, time_array, number_repetitions,plot_color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_trajectory = 0\n",
    "#list_ribosome_trajectories, list_occupancy_output, matrix_intensity_first_signa_RT, matrix_intensity_second_signa_RT \n",
    "ribosome_trajectories = list_ribosome_trajectories[selected_trajectory]    \n",
    "ribosome_trajectories = ribosome_trajectories[:,:]\n",
    "intensity_vector_first_signal = matrix_intensity_first_signal_RT[selected_trajectory,:]\n",
    "if second_probe_position_vector is not None:\n",
    "    intensity_vector_second_signal = matrix_intensity_second_signal_RT[selected_trajectory,:]\n",
    "else:\n",
    "    intensity_vector_second_signal = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ki = str(ki).replace('.','_')\n",
    "str_k = str(global_elongation_rate).replace('.','_')\n",
    "fileNameGif = 'simulation_'+plasmid_name+'_ke_'+str_k+'_ki_'+str_ki \n",
    "color = 'lightgreen'\n",
    "plot_RibosomeMovement_and_Microscope(ribosome_trajectories, intensity_vector_first_signal, tag_positions_first_probe_vector, SecondIntensityVector=intensity_vector_second_signal, second_probePositions=tag_positions_second_probe_vector,FrameVelocity=10,fileNameGif=fileNameGif,color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating missing data \n",
    "simulate_missing_data = False\n",
    "if simulate_missing_data:\n",
    "    array_simulated,_ = mi.Utilities().simulate_missing_data(matrix_intensity_first_signal_RT,matrix2= None, percentage_to_remove_data=5,replace_with='nan')\n",
    "    array_simulated  = mi.Utilities().shift_trajectories(array_simulated, )\n",
    "else:\n",
    "    array_simulated = matrix_intensity_first_signal_RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_parameters (gene_length, tag_positions_first_probe_vector, g_0, dwell_time):\n",
    "    print ('------------------------------------')\n",
    "    #ke_calculated_ch0 =  np.round( (gene_length-np.max(tag_positions_first_probe_vector)/2) /dwell_time_ch0  , 2)\n",
    "    ke_calculated_ch0 =  np.round( gene_length /dwell_time , 2)\n",
    "    ke_calculated_ch0_corrected =  np.round( (gene_length-(np.max(tag_positions_first_probe_vector))) /dwell_time  , 2)\n",
    "    print ('------------------------------------')\n",
    "    print('Elongation rates: ')\n",
    "    print('Calculated ch0', ke_calculated_ch0 , ' Corrected: ',ke_calculated_ch0_corrected, )\n",
    "    print ('------------------------------------')\n",
    "    print('Initiation rates: ')\n",
    "    # initiation rate\n",
    "    ki_calculated = np.round( 1/ (g_0* dwell_time), 3)\n",
    "    print('Calculated', ki_calculated, ' 1/sec')\n",
    "    print ('------------------------------------')\n",
    "    print('Ribosomal density: ')\n",
    "    # initiation rate\n",
    "    #ribosomal_density = ki_calculated* ke_calculated_ch0_corrected\n",
    "    ribosomal_density = np.round( (gene_length/ke_calculated_ch0_corrected) *ki_calculated , 1)\n",
    "    print('Calculated', np.round(ribosomal_density,1) , ' average number of ribosomes per RNA')\n",
    "    print ('------------------------------------')\n",
    "    print('Ribosomal occurrence: ')\n",
    "    # initiation rate\n",
    "    ribsomal_space = 1/ki_calculated \n",
    "    print('Calculated', np.round(ribsomal_space,2) , ' seconds between ribosome initiation')\n",
    "    print ('------------------------------------')\n",
    "    return None\n",
    "\n",
    "print_parameters(gene_length, tag_positions_first_probe_vector, g_0=0.08, dwell_time=279)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_correlation, std_correlation, lags, correlations_array, dwell_time = mi.Correlation(\n",
    "    primary_data= matrix_intensity_first_signal_RT, #intensity_vector_first_signal.reshape(1, -1),  # SHAPE 2D\n",
    "    #secondary_data=amplitude_vector_delayed.reshape(-1, 1),  # SHAPE 2D\n",
    "    #max_lag=180, \n",
    "    nan_handling='forward_fill',\n",
    "    shift_data=True,\n",
    "    return_full=False,\n",
    "    time_interval_between_frames_in_seconds=1,\n",
    "    use_bootstrap=True,\n",
    "    show_plot=True,\n",
    "    start_lag=0,\n",
    "    fit_type='linear',\n",
    "    de_correlation_threshold=0.005,\n",
    "    correct_baseline=True,\n",
    "    use_linear_projection_for_lag_0=False,\n",
    "    save_plots=False,\n",
    "    use_global_mean=False,\n",
    "    remove_outliers=False,\n",
    "    MAD_THRESHOLD_FACTOR=6,\n",
    "    plot_individual_trajectories=False,\n",
    "    #x_axes_min_max_list_values=[-120,120],\n",
    "    #y_axes_min_max_list_values=[-1,2],\n",
    "    multi_tau=False,\n",
    "    plot_title=None\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Stop here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Correlation:\n",
    "    \"\"\"\n",
    "    A class for calculating the autocorrelation or cross-correlation of datasets.\n",
    "\n",
    "    Attributes:\n",
    "        primary_data (np.ndarray): Primary dataset for autocorrelation, shape [sample, time].\n",
    "        secondary_data (np.ndarray, optional): Secondary dataset for cross-correlation, same shape as primary.\n",
    "        max_lag (int, optional): Maximum lag to compute correlation, defaults to half time series length.\n",
    "        nan_handling (str, optional): Strategy to handle NaN values. Options: 'zeros', 'mean', 'forward_fill', 'ignore'.\n",
    "        return_full (bool, optional): Whether to return the full correlation array or only positive lags.\n",
    "        use_bootstrap (bool, optional): Whether to use bootstrap for error estimation.\n",
    "        shift_data (bool, optional): Whether to shift data based on leading NaNs.\n",
    "        show_plot (bool, optional): Whether to display the plot.\n",
    "        save_plots (bool, optional): Whether to save the plots.\n",
    "        plot_name (str, optional): Name of the plot file.\n",
    "        time_interval_between_frames_in_seconds (int, optional): Time interval between frames.\n",
    "        index_max_lag_for_fit (int, optional): Index for maximum lag for fitting.\n",
    "        color_channel (int, optional): Color channel for plotting.\n",
    "        start_lag (int, optional): Starting lag for plateau finding.\n",
    "        line_color (str, optional): Color of the plot line.\n",
    "        plot_title (str, optional): Title of the plot.\n",
    "        fit_type (str, optional): Type of fit for the plot.\n",
    "        de_correlation_threshold (float, optional): Threshold for decorrelation.\n",
    "        use_linear_projection_for_lag_0 (bool, optional): Whether to use linear projection for lag 0.\n",
    "        correct_baseline (bool, optional): If True, subtract baseline from mean correlation.\n",
    "        use_global_mean (bool, optional): If True, use a global mean for correlation normalization.\n",
    "        use_normalization_factor (bool, optional): If True, multiply correlation by 1/(global_means * #time_points).\n",
    "        remove_outliers (bool, optional): If True, remove \u201cextreme\u201d outlier trajectories before computing mean.\n",
    "        MAD_THRESHOLD_FACTOR (float, optional): Threshold factor for outlier removal.\n",
    "        multi_tau (bool, optional): If True, use multi-tau algorithm for correlation (non-uniform, positive lags only).\n",
    "\n",
    "    Methods:\n",
    "        run():\n",
    "            Executes the correlation computation based on initialized settings.\n",
    "\n",
    "            Returns:\n",
    "                (mean_correlation, error_correlation, lags, correlations_array, dwell_time)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        primary_data,\n",
    "        secondary_data=None,\n",
    "        max_lag=None,\n",
    "        nan_handling='zeros',\n",
    "        return_full=True,\n",
    "        use_bootstrap=True,\n",
    "        shift_data=False,\n",
    "        show_plot=False,\n",
    "        save_plots=False,\n",
    "        plot_name='temp_AC.png',\n",
    "        time_interval_between_frames_in_seconds=1,\n",
    "        index_max_lag_for_fit=None,\n",
    "        color_channel=0,\n",
    "        start_lag=0,\n",
    "        line_color='blue',\n",
    "        correct_baseline=False,\n",
    "        baseline_offset=None,\n",
    "        use_global_mean=False,\n",
    "        plot_title=None,\n",
    "        fit_type='linear',\n",
    "        de_correlation_threshold=0.01,\n",
    "        use_linear_projection_for_lag_0=True,\n",
    "        normalize_plot_with_g0=False,\n",
    "        remove_outliers=True,\n",
    "        MAD_THRESHOLD_FACTOR=6.0,\n",
    "        plot_individual_trajectories=False,\n",
    "        y_axes_min_max_list_values=None,\n",
    "        x_axes_min_max_list_values=None,\n",
    "        multi_tau=False\n",
    "    ):\n",
    "        def shift_and_fill(data1, data2=None, min_nan_threshold=3, fill_with_nans=True):\n",
    "            \"\"\"\n",
    "            Remove leading NaNs beyond a threshold and shift arrays left, filling the end with NaNs or zeros.\n",
    "            \"\"\"\n",
    "            if data1.ndim != 1:\n",
    "                raise ValueError(\"Both data1 and data2 must be 1D arrays.\")\n",
    "            nan_count = 0\n",
    "            for value in data1:\n",
    "                if np.isnan(value):\n",
    "                    nan_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            if nan_count >= min_nan_threshold:\n",
    "                fill_value = np.nan if fill_with_nans else 0\n",
    "                new_data1 = np.full_like(data1, fill_value)\n",
    "                new_data1[: len(data1) - nan_count] = data1[nan_count:]\n",
    "                if data2 is not None:\n",
    "                    new_data2 = np.full_like(data2, fill_value)\n",
    "                    new_data2[: len(data2) - nan_count] = data2[nan_count:]\n",
    "                else:\n",
    "                    new_data2 = None\n",
    "                return new_data1, new_data2\n",
    "            return data1, data2\n",
    "\n",
    "        if shift_data:\n",
    "            primary_data_shifted = np.zeros_like(primary_data)\n",
    "            secondary_data_shifted = np.zeros_like(secondary_data) if secondary_data is not None else None\n",
    "            for i in range(primary_data.shape[0]):\n",
    "                if secondary_data is None:\n",
    "                    primary_data_shifted[i, :], _ = shift_and_fill(primary_data[i, :], None, min_nan_threshold=2)\n",
    "                else:\n",
    "                    primary_data_shifted[i, :], secondary_data_shifted[i, :] = shift_and_fill(primary_data[i, :], secondary_data[i, :], min_nan_threshold=2)\n",
    "            primary_data = primary_data_shifted\n",
    "            if secondary_data is not None:\n",
    "                secondary_data = secondary_data_shifted\n",
    "\n",
    "        # Store attributes\n",
    "        self.primary_data = primary_data\n",
    "        self.secondary_data = secondary_data\n",
    "        self.max_lag = max_lag\n",
    "        self.nan_handling = nan_handling\n",
    "        self.return_full = return_full\n",
    "        self.use_bootstrap = use_bootstrap\n",
    "        self.BOOTSTRAP_ITERATIONS = 1000\n",
    "        self.time_interval_between_frames_in_seconds = float(time_interval_between_frames_in_seconds)\n",
    "        self.index_max_lag_for_fit = index_max_lag_for_fit\n",
    "        self.plot_name = plot_name\n",
    "        self.save_plots = save_plots\n",
    "        self.show_plot = show_plot\n",
    "        self.color_channel = color_channel\n",
    "        self.start_lag = start_lag\n",
    "        self.line_color = line_color\n",
    "        self.plot_title = plot_title\n",
    "        self.fit_type = fit_type\n",
    "        self.de_correlation_threshold = de_correlation_threshold\n",
    "        self.use_linear_projection_for_lag_0 = use_linear_projection_for_lag_0\n",
    "        self.normalize_plot_with_g0 = normalize_plot_with_g0\n",
    "        self.correct_baseline = correct_baseline\n",
    "        if baseline_offset is None:\n",
    "            # Default: use half the time series length as baseline offset for fitting baseline\n",
    "            self.baseline_offset = int(primary_data.shape[1] // 2)\n",
    "        else:\n",
    "            self.baseline_offset = baseline_offset\n",
    "        if correct_baseline:\n",
    "            plot_individual_trajectories = False\n",
    "            print('Baseline correction is enabled. Plotting individual trajectories is disabled due to baseline correction.')\n",
    "        self.use_global_mean = use_global_mean\n",
    "        self.remove_outliers = remove_outliers\n",
    "        self.MAD_THRESHOLD_FACTOR = MAD_THRESHOLD_FACTOR\n",
    "        self.plot_individual_trajectories = plot_individual_trajectories\n",
    "        self.y_axes_min_max_list_values = y_axes_min_max_list_values\n",
    "        self.x_axes_min_max_list_values = x_axes_min_max_list_values\n",
    "        self.multi_tau = multi_tau\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the correlation calculations with optional bootstrap error estimation.\n",
    "        \"\"\"\n",
    "        if self.max_lag is None:\n",
    "            self.max_lag = self.primary_data.shape[1] - 1\n",
    "        else:\n",
    "            if self.max_lag >= self.primary_data.shape[1]:\n",
    "                raise ValueError(\"Max lag cannot be greater than the length of the time series.\")\n",
    "\n",
    "        # Helper functions\n",
    "        def trim_nans_from_edges(data):\n",
    "            mask = ~np.isnan(data)\n",
    "            if not np.any(mask):\n",
    "                return np.array([])\n",
    "            start_idx = np.argmax(mask)\n",
    "            end_idx = len(mask) - np.argmax(mask[::-1])\n",
    "            return data[start_idx:end_idx]\n",
    "\n",
    "        # Prepare forward fill function if needed\n",
    "        if self.nan_handling == \"forward_fill\":\n",
    "            def forward_fill_func(data):\n",
    "                not_nan = ~np.isnan(data)\n",
    "                if not np.any(not_nan):\n",
    "                    return np.array([])\n",
    "                first_valid_index = np.argmax(not_nan)\n",
    "                last_valid_index = len(data) - np.argmax(not_nan[::-1]) - 1\n",
    "                trimmed = data[first_valid_index:last_valid_index + 1]\n",
    "                mask_ff = np.isnan(trimmed)\n",
    "                idx = np.where(~mask_ff, np.arange(len(trimmed)), 0)\n",
    "                np.maximum.accumulate(idx, out=idx)\n",
    "                filled = trimmed[idx]\n",
    "                result = np.full_like(data, np.nan)\n",
    "                result[first_valid_index:last_valid_index + 1] = filled\n",
    "                return result\n",
    "            local_forward_fill = forward_fill_func\n",
    "        else:\n",
    "            local_forward_fill = lambda arr: arr\n",
    "\n",
    "        # Global means for normalization if needed\n",
    "        global_mean_data1 = np.nanmean(self.primary_data)\n",
    "        global_mean_data2 = np.nanmean(self.secondary_data) if self.secondary_data is not None else global_mean_data1\n",
    "\n",
    "        if not self.multi_tau:\n",
    "            # Linear correlation for each sample (symmetric output)\n",
    "            def process_sample_linear(i):\n",
    "                try:\n",
    "                    data1 = trim_nans_from_edges(self.primary_data[i, :])\n",
    "                    data2 = trim_nans_from_edges(self.secondary_data[i, :]) if self.secondary_data is not None else None\n",
    "                    if data2 is None:\n",
    "                        data2 = data1\n",
    "                    # Handle NaNs according to strategy\n",
    "                    if self.nan_handling == \"mean\":\n",
    "                        mean_val1 = np.nanmean(data1) if len(data1) > 0 else 0.0\n",
    "                        mean_val2 = np.nanmean(data2) if len(data2) > 0 else 0.0\n",
    "                        data1 = np.nan_to_num(data1, nan=mean_val1)\n",
    "                        data2 = np.nan_to_num(data2, nan=mean_val2)\n",
    "                    elif self.nan_handling == \"forward_fill\":\n",
    "                        data1 = local_forward_fill(data1)\n",
    "                        data2 = local_forward_fill(data2)\n",
    "                    elif self.nan_handling == \"ignore\":\n",
    "                        valid_mask = ~np.isnan(data1) & ~np.isnan(data2)\n",
    "                        data1 = data1[valid_mask]\n",
    "                        data2 = data2[valid_mask]\n",
    "                    elif self.nan_handling == \"zeros\":\n",
    "                        data1 = np.nan_to_num(data1)\n",
    "                        data2 = np.nan_to_num(data2)\n",
    "                    effective_points = np.sum(~np.isnan(data1))\n",
    "                    if effective_points < 1:\n",
    "                        return np.full(2 * self.max_lag + 1, np.nan)\n",
    "                    # Center data by mean\n",
    "                    if self.use_global_mean:\n",
    "                        local_mean1 = global_mean_data1\n",
    "                        local_mean2 = global_mean_data2\n",
    "                    else:\n",
    "                        local_mean1 = np.nanmean(data1)\n",
    "                        local_mean2 = np.nanmean(data2)\n",
    "                    cdata1 = data1 - local_mean1\n",
    "                    cdata2 = data2 - local_mean2\n",
    "                    # Remove any residual NaNs\n",
    "                    cdata1 = cdata1[~np.isnan(cdata1)]\n",
    "                    cdata2 = cdata2[~np.isnan(cdata2)]\n",
    "                    if len(cdata1) == 0 or len(cdata2) == 0:\n",
    "                        return np.full(2 * self.max_lag + 1, np.nan)\n",
    "                    N = len(cdata1)\n",
    "                    # Compute raw full cross-correlation\n",
    "                    raw_corr = np.correlate(cdata1, cdata2, mode=\"full\")\n",
    "                    mid = N - 1\n",
    "                    min_overlap = max(5, int(0.2 * N))\n",
    "                    final_corr = np.empty_like(raw_corr, dtype=np.float64)\n",
    "                    for j in range(len(raw_corr)):\n",
    "                        lag = j - mid\n",
    "                        overlap = N - abs(lag)\n",
    "                        if overlap < min_overlap:\n",
    "                            final_corr[j] = np.nan\n",
    "                            continue\n",
    "                        if lag >= 0:\n",
    "                            seg1 = data1[:N - lag]\n",
    "                            seg2 = data2[lag:]\n",
    "                        else:\n",
    "                            seg1 = data1[-lag:]\n",
    "                            seg2 = data2[:N + lag]\n",
    "                        local_norm = np.nanmean(seg1) * np.nanmean(seg2)\n",
    "                        if local_norm == 0:\n",
    "                            final_corr[j] = np.nan\n",
    "                        else:\n",
    "                            final_corr[j] = (raw_corr[j] / overlap) / local_norm\n",
    "                    mid_point = len(final_corr) // 2\n",
    "                    desired_length = 2 * self.max_lag + 1\n",
    "                    current_length = len(final_corr)\n",
    "                    if current_length < desired_length:\n",
    "                        # Pad with NaNs if needed\n",
    "                        out = np.full(desired_length, np.nan)\n",
    "                        start_idx = (desired_length - current_length) // 2\n",
    "                        out[start_idx:start_idx + current_length] = final_corr\n",
    "                        return out\n",
    "                    else:\n",
    "                        start_idx = mid_point - self.max_lag\n",
    "                        end_idx = mid_point + self.max_lag + 1\n",
    "                        return final_corr[start_idx:end_idx]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in process_sample_linear for sample {i}: {e}\")\n",
    "                    return np.full(2 * self.max_lag + 1, np.nan)\n",
    "\n",
    "            correlations_array = np.array(\n",
    "                Parallel(n_jobs=-1)(delayed(process_sample_linear)(i) for i in range(self.primary_data.shape[0])),\n",
    "                dtype=np.float64\n",
    "            )\n",
    "        else:\n",
    "            # Multi-tau correlation for each sample (positive lags, non-uniform spacing)\n",
    "            # Set parameter m (channels per stage after initial). m=8 gives 16 initial points, then groups of 8.\n",
    "            m = 8\n",
    "            N0 = self.primary_data.shape[1]\n",
    "            global_lags_idx = []\n",
    "            current_length = N0\n",
    "            dt_factor = 1\n",
    "            stage = 0\n",
    "            while True:\n",
    "                if stage == 0:\n",
    "                    start_i = 0\n",
    "                    end_i = min(2 * m - 1, self.max_lag, current_length - 1)\n",
    "                else:\n",
    "                    start_i = m\n",
    "                    end_i = min(2 * m - 1, int(self.max_lag // dt_factor), current_length - 1)\n",
    "                if start_i > end_i:\n",
    "                    break\n",
    "                for i_val in range(start_i, end_i + 1):\n",
    "                    global_lags_idx.append(i_val * dt_factor)\n",
    "                if end_i < 2 * m - 1 or (end_i * dt_factor) >= self.max_lag or current_length < 2:\n",
    "                    break\n",
    "                new_length = current_length // 2\n",
    "                if new_length < 1:\n",
    "                    break\n",
    "                current_length = new_length\n",
    "                dt_factor *= 2\n",
    "                stage += 1\n",
    "            global_lags_idx = sorted(set(global_lags_idx))\n",
    "            global_lags_idx = np.array(global_lags_idx, dtype=int)\n",
    "            idx_map = {lag: idx for idx, lag in enumerate(global_lags_idx)}\n",
    "\n",
    "            def process_sample_multi_tau(i):\n",
    "                try:\n",
    "                    data1 = trim_nans_from_edges(self.primary_data[i, :])\n",
    "                    data2 = trim_nans_from_edges(self.secondary_data[i, :]) if self.secondary_data is not None else None\n",
    "                    if data2 is None:\n",
    "                        data2 = data1\n",
    "                    # Handle NaNs according to strategy\n",
    "                    if self.nan_handling == \"mean\":\n",
    "                        mean_val1 = np.nanmean(data1) if len(data1) > 0 else 0.0\n",
    "                        mean_val2 = np.nanmean(data2) if len(data2) > 0 else 0.0\n",
    "                        data1 = np.nan_to_num(data1, nan=mean_val1)\n",
    "                        data2 = np.nan_to_num(data2, nan=mean_val2)\n",
    "                    elif self.nan_handling == \"forward_fill\":\n",
    "                        data1 = local_forward_fill(data1)\n",
    "                        data2 = local_forward_fill(data2)\n",
    "                    elif self.nan_handling == \"ignore\":\n",
    "                        valid_mask = ~np.isnan(data1) & ~np.isnan(data2)\n",
    "                        data1 = data1[valid_mask]\n",
    "                        data2 = data2[valid_mask]\n",
    "                    elif self.nan_handling == \"zeros\":\n",
    "                        data1 = np.nan_to_num(data1)\n",
    "                        data2 = np.nan_to_num(data2)\n",
    "                    effective_points = np.sum(~np.isnan(data1))\n",
    "                    if effective_points < 1:\n",
    "                        # Return all-NaN array of global length\n",
    "                        return np.full(len(global_lags_idx), np.nan)\n",
    "                    # Center data by mean\n",
    "                    if self.use_global_mean:\n",
    "                        local_mean1 = global_mean_data1\n",
    "                        local_mean2 = global_mean_data2\n",
    "                    else:\n",
    "                        local_mean1 = np.nanmean(data1)\n",
    "                        local_mean2 = np.nanmean(data2)\n",
    "                    cdata1 = data1 - local_mean1\n",
    "                    cdata2 = data2 - local_mean2\n",
    "                    # Remove any residual NaNs\n",
    "                    mask_valid = ~np.isnan(cdata1) & ~np.isnan(cdata2)\n",
    "                    cdata1 = cdata1[mask_valid]\n",
    "                    cdata2 = cdata2[mask_valid]\n",
    "                    data1_valid = data1[mask_valid]\n",
    "                    data2_valid = data2[mask_valid]\n",
    "                    current_N = len(cdata1)\n",
    "                    if current_N == 0:\n",
    "                        return np.full(len(global_lags_idx), np.nan)\n",
    "                    # Multi-tau loop\n",
    "                    output_corr = np.full(len(global_lags_idx), np.nan, dtype=np.float64)\n",
    "                    current_data_raw1 = data1_valid.copy()\n",
    "                    current_data_raw2 = data2_valid.copy()\n",
    "                    current_cdata1 = cdata1.copy()\n",
    "                    current_cdata2 = cdata2.copy()\n",
    "                    dt_factor = 1\n",
    "                    stage = 0\n",
    "                    while True:\n",
    "                        if stage == 0:\n",
    "                            start_i = 0\n",
    "                            end_i = min(2 * m - 1, self.max_lag, current_N - 1)\n",
    "                        else:\n",
    "                            start_i = m\n",
    "                            end_i = min(2 * m - 1, int(self.max_lag // dt_factor), current_N - 1)\n",
    "                        if start_i > end_i:\n",
    "                            break\n",
    "                        min_overlap = max(5, int(0.2 * current_N))\n",
    "                        for j in range(start_i, end_i + 1):\n",
    "                            overlap = current_N - j\n",
    "                            if overlap < min_overlap:\n",
    "                                continue\n",
    "                            raw_sum = np.nansum(current_cdata1[:current_N - j] * current_cdata2[j:current_N])\n",
    "                            seg1 = current_data_raw1[:current_N - j]\n",
    "                            seg2 = current_data_raw2[j:current_N]\n",
    "                            local_norm = np.nanmean(seg1) * np.nanmean(seg2)\n",
    "                            if local_norm == 0:\n",
    "                                corr_val = np.nan\n",
    "                            else:\n",
    "                                corr_val = (raw_sum / overlap) / local_norm\n",
    "                            lag_index = j * dt_factor\n",
    "                            if lag_index in idx_map:\n",
    "                                output_corr[idx_map[lag_index]] = corr_val\n",
    "                        if end_i < 2 * m - 1 or (end_i * dt_factor) >= self.max_lag or current_N < 2:\n",
    "                            break\n",
    "                        new_length = current_N // 2\n",
    "                        if new_length < 1:\n",
    "                            break\n",
    "                        # Downsample data\n",
    "                        if self.secondary_data is None:\n",
    "                            new_data_raw1 = 0.5 * (current_data_raw1[:2 * new_length:2] + current_data_raw1[1:2 * new_length:2])\n",
    "                            new_data_raw2 = new_data_raw1  # autocorrelation\n",
    "                        else:\n",
    "                            new_data_raw1 = 0.5 * (current_data_raw1[:2 * new_length:2] + current_data_raw1[1:2 * new_length:2])\n",
    "                            new_data_raw2 = 0.5 * (current_data_raw2[:2 * new_length:2] + current_data_raw2[1:2 * new_length:2])\n",
    "                        if self.use_global_mean:\n",
    "                            new_mean1 = global_mean_data1\n",
    "                            new_mean2 = global_mean_data2\n",
    "                        else:\n",
    "                            new_mean1 = np.nanmean(new_data_raw1)\n",
    "                            new_mean2 = np.nanmean(new_data_raw2) if self.secondary_data is not None else new_mean1\n",
    "                        new_cdata1 = new_data_raw1 - new_mean1\n",
    "                        new_cdata2 = new_data_raw2 - new_mean2\n",
    "                        # Update for next stage\n",
    "                        current_data_raw1 = new_data_raw1\n",
    "                        current_data_raw2 = new_data_raw2\n",
    "                        current_cdata1 = new_cdata1 = new_cdata1  # (just to use consistent naming; not needed separately)\n",
    "                        current_cdata2 = new_cdata2 = new_cdata2\n",
    "                        current_N = new_length\n",
    "                        dt_factor *= 2\n",
    "                        stage += 1\n",
    "                    return output_corr\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in process_sample_multi_tau for sample {i}: {e}\")\n",
    "                    return np.full(len(global_lags_idx), np.nan)\n",
    "\n",
    "            correlations_array = np.array(\n",
    "                Parallel(n_jobs=-1)(delayed(process_sample_multi_tau)(i) for i in range(self.primary_data.shape[0])),\n",
    "                dtype=np.float64\n",
    "            )\n",
    "\n",
    "        # Remove outlier trajectories if required\n",
    "        if self.remove_outliers and correlations_array.size > 0:\n",
    "            traj_means = np.nanmean(correlations_array, axis=1)\n",
    "            median_mean = np.nanmedian(traj_means)\n",
    "            mad = np.nanmedian(np.abs(traj_means - median_mean))\n",
    "            if mad == 0 or np.isnan(mad):\n",
    "                keep_mask = np.ones_like(traj_means, dtype=bool)\n",
    "            else:\n",
    "                keep_mask = np.abs(traj_means - median_mean) < self.MAD_THRESHOLD_FACTOR * mad\n",
    "            num_removed = np.sum(~keep_mask)\n",
    "            num_total = len(traj_means)\n",
    "            if num_removed > 0:\n",
    "                print(f\"Warning: Removed {num_removed} outlier trajectories (out of {num_total}) based on a threshold of {self.MAD_THRESHOLD_FACTOR} MAD from the median mean correlation.\")\n",
    "            correlations_array = correlations_array[keep_mask, :]\n",
    "\n",
    "        # If all data removed or no valid points\n",
    "        if correlations_array.shape[0] == 0:\n",
    "            length = correlations_array.shape[1] if correlations_array.ndim > 1 else (len(global_lags_idx) if self.multi_tau else (2 * self.max_lag + 1))\n",
    "            mean_correlation = np.full(length, np.nan)\n",
    "            error_correlation = np.full_like(mean_correlation, np.nan)\n",
    "            if not self.multi_tau:\n",
    "                lags = np.arange(-self.max_lag, self.max_lag + 1) * self.time_interval_between_frames_in_seconds\n",
    "            else:\n",
    "                lags = (global_lags_idx if 'global_lags_idx' in locals() else np.arange(0, self.max_lag + 1)) * self.time_interval_between_frames_in_seconds\n",
    "            return mean_correlation, error_correlation, lags, correlations_array, None\n",
    "\n",
    "        # Compute mean correlation and error bars\n",
    "        mean_correlation = np.nanmean(correlations_array, axis=0)\n",
    "        # Correct baseline via exponential fit if requested\n",
    "        if self.correct_baseline:\n",
    "            L = len(mean_correlation) - 1\n",
    "            start_idx_fit = 2\n",
    "            time_range = int(L * 0.99)\n",
    "            if time_range <= start_idx_fit:\n",
    "                time_range = start_idx_fit + 1\n",
    "            if not self.multi_tau:\n",
    "                # Construct positive lags array (assuming mean_correlation corresponds to lag 0..max_lag if return_full=False, or symmetrical otherwise)\n",
    "                if self.return_full:\n",
    "                    # If still symmetric, convert to positive lags portion for fitting\n",
    "                    lags_array = np.arange(0, L + 1) * self.time_interval_between_frames_in_seconds\n",
    "                else:\n",
    "                    lags_array = np.arange(0, len(mean_correlation)) * self.time_interval_between_frames_in_seconds\n",
    "            else:\n",
    "                lags_array = global_lags_idx * self.time_interval_between_frames_in_seconds\n",
    "            y_fit = mean_correlation[start_idx_fit:time_range]\n",
    "            t_fit = lags_array[start_idx_fit:time_range]\n",
    "            B_guess = y_fit[-1] if len(y_fit) > 0 else 0.0\n",
    "            A_guess = mean_correlation[0] - B_guess\n",
    "            tau_guess = (t_fit[-1] - t_fit[0]) / 2.0 if len(t_fit) > 1 else 1.0\n",
    "            initial_guess = [A_guess, tau_guess, B_guess]\n",
    "            lower_bounds = [0, 1e-6, np.min(y_fit) if len(y_fit) > 0 else 0.0]\n",
    "            upper_bounds = [np.inf, np.inf, mean_correlation[0]]\n",
    "            mask_fit = np.isfinite(y_fit) & np.isfinite(t_fit)\n",
    "            t_clean = t_fit[mask_fit]\n",
    "            y_clean = y_fit[mask_fit]\n",
    "            if len(t_clean) < 3:\n",
    "                warnings.warn(f\"Too few valid points ({len(t_clean)}) for exponential fit\u2014using fallback.\")\n",
    "                fitted_B = np.nanpercentile(y_fit, 10) if len(y_fit) > 0 else 0.0\n",
    "            else:\n",
    "                try:\n",
    "                    popt, _ = curve_fit(lambda t, A, tau, B: A * np.exp(-t / tau) + B,\n",
    "                                        t_clean, y_clean, p0=initial_guess,\n",
    "                                        bounds=(lower_bounds, upper_bounds),\n",
    "                                        maxfev=10000)\n",
    "                    fitted_B = popt[2]\n",
    "                    print(f\"Exponential fit parameters: A={popt[0]:.3g}, \u03c4={popt[1]:.3g}, B={popt[2]:.3g}\")\n",
    "                except Exception as e:\n",
    "                    warnings.warn(f\"Exp fit failed ({type(e).__name__}: {e}) \u2192 using 10th percentile fallback.\")\n",
    "                    fitted_B = np.nanpercentile(y_fit, 10) if len(y_fit) > 0 else 0.0\n",
    "            mean_correlation = mean_correlation - fitted_B\n",
    "\n",
    "        num_kept = correlations_array.shape[0]\n",
    "        if self.use_bootstrap and num_kept > 1:\n",
    "            def single_bootstrap_iteration(_):\n",
    "                rng = np.random.default_rng()\n",
    "                indices = rng.choice(num_kept, size=num_kept, replace=True)\n",
    "                sample = correlations_array[indices, :]\n",
    "                m = np.nanmean(sample, axis=0)\n",
    "                if self.correct_baseline:\n",
    "                    if not self.multi_tau:\n",
    "                        center_idx = self.max_lag\n",
    "                        offset = min(self.baseline_offset, center_idx)\n",
    "                        neg_region = m[center_idx - offset : center_idx]\n",
    "                        pos_region = m[center_idx + 1 : center_idx + 1 + offset]\n",
    "                        baseline_value = np.nanpercentile(np.concatenate([neg_region, pos_region]), 10)\n",
    "                        m = m - baseline_value\n",
    "                    else:\n",
    "                        offset = min(self.baseline_offset, len(m))\n",
    "                        tail_region = m[-offset:] if offset > 0 else m\n",
    "                        baseline_value = np.nanpercentile(tail_region, 10)\n",
    "                        m = m - baseline_value\n",
    "                return m\n",
    "            all_means = np.array(\n",
    "                Parallel(n_jobs=-1)(delayed(single_bootstrap_iteration)(_) for _ in range(self.BOOTSTRAP_ITERATIONS)),\n",
    "                dtype=np.float64\n",
    "            )\n",
    "            error_correlation = np.nanstd(all_means, axis=0)\n",
    "        else:\n",
    "            error_correlation = np.nanstd(correlations_array, axis=0) / np.sqrt(num_kept)\n",
    "\n",
    "        # Construct lags array (in seconds)\n",
    "        if not self.multi_tau:\n",
    "            lags = np.arange(-self.max_lag, self.max_lag + 1) * self.time_interval_between_frames_in_seconds\n",
    "        else:\n",
    "            lags = global_lags_idx * self.time_interval_between_frames_in_seconds\n",
    "\n",
    "        # Linear projection adjustment for lag=0\n",
    "        if self.use_linear_projection_for_lag_0:\n",
    "            if not self.multi_tau:\n",
    "                center_idx = self.max_lag\n",
    "                if self.secondary_data is None:\n",
    "                    # Autocorrelation: use negative side to project to 0\n",
    "                    if center_idx - 6 >= 0 and center_idx - 1 >= 0:\n",
    "                        x = lags[center_idx - 6 : center_idx - 1]\n",
    "                        y = mean_correlation[center_idx - 5 : center_idx]\n",
    "                        if len(x) > 1 and np.all(np.isfinite(y)):\n",
    "                            _, intercept, _, _, _ = linregress(x, y)\n",
    "                            mean_correlation[center_idx] = intercept\n",
    "                    if center_idx < len(error_correlation):\n",
    "                        error_correlation[center_idx] = 0\n",
    "                else:\n",
    "                    # Cross-correlation: project both sides and take max\n",
    "                    if center_idx - 6 >= 0 and center_idx - 1 >= 0:\n",
    "                        x_bef = lags[center_idx - 6 : center_idx - 1]\n",
    "                        y_bef = mean_correlation[center_idx - 5 : center_idx]\n",
    "                        corr_before = linregress(x_bef, y_bef).intercept if len(x_bef) > 1 and np.all(np.isfinite(y_bef)) else mean_correlation[center_idx]\n",
    "                    else:\n",
    "                        corr_before = mean_correlation[center_idx]\n",
    "                    if center_idx + 6 < len(mean_correlation):\n",
    "                        x_aft = lags[center_idx + 1 : center_idx + 6]\n",
    "                        y_aft = mean_correlation[center_idx + 1 : center_idx + 6]\n",
    "                        corr_after = linregress(x_aft, y_aft).intercept if len(x_aft) > 1 and np.all(np.isfinite(y_aft)) else mean_correlation[center_idx]\n",
    "                    else:\n",
    "                        corr_after = mean_correlation[center_idx]\n",
    "                    mean_correlation[center_idx] = np.nanmax([corr_before, corr_after])\n",
    "                    if center_idx < len(error_correlation):\n",
    "                        error_correlation[center_idx] = 0\n",
    "            else:\n",
    "                if self.secondary_data is None:\n",
    "                    # Autocorrelation multi-tau: use first few lags to project to 0\n",
    "                    if len(lags) > 5:\n",
    "                        x = lags[1:6]\n",
    "                        y = mean_correlation[1:6]\n",
    "                        if len(x) > 1 and np.all(np.isfinite(y)):\n",
    "                            _, intercept, _, _, _ = linregress(x, y)\n",
    "                            mean_correlation[0] = intercept\n",
    "                    if len(error_correlation) > 0:\n",
    "                        error_correlation[0] = 0\n",
    "                else:\n",
    "                    # Cross-correlation multi-tau: no adjustment (lack negative lags)\n",
    "                    pass\n",
    "\n",
    "        # For linear correlation, handle return_full flag (for positive lags only)\n",
    "        if not self.multi_tau and not self.return_full:\n",
    "            mean_correlation = mean_correlation[self.max_lag:]\n",
    "            error_correlation = error_correlation[self.max_lag:]\n",
    "            correlations_array = correlations_array[:, self.max_lag:]\n",
    "            lags = lags[self.max_lag:]\n",
    "\n",
    "        dwell_time = None\n",
    "        if self.show_plot:\n",
    "            if self.secondary_data is None:\n",
    "                dwell_time = mi.Plots().plot_autocorrelation(\n",
    "                    mean_correlation=mean_correlation,\n",
    "                    error_correlation=error_correlation,\n",
    "                    lags=lags,\n",
    "                    correlations_array=correlations_array,\n",
    "                    time_interval_between_frames_in_seconds=self.time_interval_between_frames_in_seconds,\n",
    "                    index_max_lag_for_fit=self.index_max_lag_for_fit,\n",
    "                    start_lag=self.start_lag,\n",
    "                    plot_name=self.plot_name,\n",
    "                    save_plots=self.save_plots,\n",
    "                    line_color=self.line_color,\n",
    "                    plot_title=self.plot_title,\n",
    "                    fit_type=self.fit_type,\n",
    "                    de_correlation_threshold=self.de_correlation_threshold,\n",
    "                    normalize_plot_with_g0=self.normalize_plot_with_g0,\n",
    "                    plot_individual_trajectories=self.plot_individual_trajectories,\n",
    "                    y_axes_min_max_list_values=self.y_axes_min_max_list_values,\n",
    "                    x_axes_min_max_list_values=self.x_axes_min_max_list_values,\n",
    "                )\n",
    "            else:\n",
    "                dwell_time = mi.Plots().plot_crosscorrelation(\n",
    "                    intensity_array_ch0=self.primary_data,\n",
    "                    intensity_array_ch1=self.secondary_data,\n",
    "                    mean_correlation=mean_correlation,\n",
    "                    error_correlation=error_correlation,\n",
    "                    lags=lags,\n",
    "                    time_interval_between_frames_in_seconds=self.time_interval_between_frames_in_seconds,\n",
    "                    plot_name=self.plot_name,\n",
    "                    save_plots=self.save_plots,\n",
    "                    line_color=self.line_color,\n",
    "                    plot_title=self.plot_title,\n",
    "                    normalize_plot_with_g0=self.normalize_plot_with_g0,\n",
    "                    y_axes_min_max_list_values=self.y_axes_min_max_list_values,\n",
    "                    x_axes_min_max_list_values=self.x_axes_min_max_list_values,\n",
    "                )\n",
    "        return mean_correlation, error_correlation, lags, correlations_array, dwell_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the temporal resolution\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ke = np.linspace(2,10,number_tested_parameters).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mean_correlation = []\n",
    "list_std_correlation = []\n",
    "list_lags = []\n",
    "for i, ke_constant in enumerate (list_ke):\n",
    "    ke = calculate_codon_elongation_rates (rna, global_elongation_rate=ke_constant)\n",
    "    ssa_array = simulate_TASEP_SSA(ki, ke, gene_length, t_max,\n",
    "                                time_interval_in_seconds=step_size_in_sec,\n",
    "                                number_repetitions=number_repetitions, \n",
    "                                first_probe_position_vector=first_probe_position_vector, \n",
    "                                second_probe_position_vector=second_probe_position_vector,\n",
    "                                burnin_time=burnin_time,\n",
    "                                constant_elongation_rate=ke_constant,\n",
    "                                fast_output=True)[2]\n",
    "    # Calculating the autocorrelation of the intensity signal\n",
    "    mean_correlation, std_correlation, lags, correlations_array, dwell_time = Correlation(primary_data=ssa_array,\n",
    "                                                                                            max_lag=None, \n",
    "                                                                                            nan_handling='forward_fill',  #forward_fill, 'ignore'\n",
    "                                                                                            shift_data=True,\n",
    "                                                                                            return_full=False,\n",
    "                                                                                            time_interval_between_frames_in_seconds=step_size_in_sec,\n",
    "                                                                                            use_bootstrap=True,\n",
    "                                                                                            show_plot=False,\n",
    "                                                                                            start_lag=0,\n",
    "                                                                                            fit_type='linear',\n",
    "                                                                                            de_correlation_threshold=0.01,\n",
    "                                                                                            correct_baseline=True,\n",
    "                                                                                            use_linear_projection_for_lag_0=True,\n",
    "                                                                                            save_plots=False,\n",
    "                                                                                            use_global_mean= False,\n",
    "                                                                                            remove_outliers = True,\n",
    "                                                                                            MAD_THRESHOLD_FACTOR = MAD_THRESHOLD_FACTOR,\n",
    "                                                                                            #high_outlier_percentile = high_outlier_percentile,\n",
    "                                                                                            #low_outlier_percentile = low_outlier_percentile,\n",
    "                                                                                            plot_individual_trajectories = False,\n",
    "                                                                                            y_axes_min_max_list_values = None, #y_axes_min_max_list_values,\n",
    "                                                                                            x_axes_min_max_list_values=None,\n",
    "                                                                                            multi_tau=True,\n",
    "                                                                                            plot_title=None).run()\n",
    "    \n",
    "    if downsample:\n",
    "        mean_correlation = mean_correlation[::downsample_factor]\n",
    "        std_correlation = std_correlation[::downsample_factor]\n",
    "        lags = lags[::downsample_factor]\n",
    "\n",
    "    list_mean_correlation.append(mean_correlation)\n",
    "    list_std_correlation.append(std_correlation)\n",
    "    list_lags.append(lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the min_max normalization to list_mean_correlation\n",
    "list_mean_correlation_normalized = list_mean_correlation.copy()\n",
    "for i, correlation in enumerate(list_mean_correlation):\n",
    "    list_mean_correlation_normalized[i] = (correlation - np.nanmin(correlation))/(np.nanmax(correlation) - np.nanmin(correlation))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the theoretical deocorrelation values for each elongation rate.\n",
    "list_theoretical_decorrelation = []\n",
    "for i, ke_constant in enumerate (list_ke):\n",
    "    list_theoretical_decorrelation.append( gene_length/ke_constant )\n",
    "print(list_theoretical_decorrelation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',    # figure background is white\n",
    "    'axes.facecolor': 'white',      # axes (plot area) background is white\n",
    "    'axes.edgecolor': 'black',      # axis spines (box) will be black\n",
    "    'axes.linewidth': 1.5,          # thicker border lines for a clear box\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': 'Arial',\n",
    "    'axes.labelsize': 14,           # axis labels: minimum 12\n",
    "    'axes.titlesize': 14,           # title font size: 16\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    # make ticks adn axels labels black fonts\n",
    "    'axes.labelcolor': 'black',\n",
    "    'text.color': 'black',\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'axes.edgecolor': 'black',\n",
    "})\n",
    "for i, mean_correlation in enumerate(list_mean_correlation_normalized):\n",
    "    ax.plot(lags, mean_correlation, label='$k_e$='+str(list_ke[i]), linewidth=2)\n",
    "    ax.fill_between(lags, mean_correlation - list_std_correlation[i], mean_correlation + list_std_correlation[i], alpha=0.2)\n",
    "ax.set_xlabel(r'$\\tau (s)$', fontsize=14)\n",
    "ax.set_ylabel(r'$G(\\tau)/G(0)$', fontdict={'fontsize': 14})\n",
    "ax.legend()\n",
    "\n",
    "# plot the line of the theoretical decorrelation as a vertical dashed line of the same color as the plot line. \n",
    "for i, theoretical_decorrelation in enumerate(list_theoretical_decorrelation):\n",
    "    ax.axvline(x=theoretical_decorrelation, color=ax.get_lines()[i].get_color(), linestyle='--',lw=1)\n",
    "# plot a circle at the theoretical decorrelation value and y =0\n",
    "for i, theoretical_decorrelation in enumerate(list_theoretical_decorrelation):\n",
    "    ax.plot(theoretical_decorrelation, 0, markersize=10, marker='o', color=ax.get_lines()[i].get_color())\n",
    "\n",
    "ax.set_xlim([-10, 1500])\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(1.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing initiation\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ki = np.round(np.linspace(0.01,0.1,number_tested_parameters),3)\n",
    "print(list_ki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mean_correlation_ki = []\n",
    "list_std_correlation_ki = []\n",
    "list_lags_ki = []\n",
    "fixed_ke = 5\n",
    "for i, ki_tested in enumerate (list_ki):\n",
    "    ke = calculate_codon_elongation_rates (rna, global_elongation_rate=fixed_ke)\n",
    "    ssa_array = simulate_TASEP_SSA(ki_tested, ke, gene_length, t_max,\n",
    "                                time_interval_in_seconds=step_size_in_sec,\n",
    "                                number_repetitions=number_repetitions, \n",
    "                                first_probe_position_vector=first_probe_position_vector, \n",
    "                                second_probe_position_vector=second_probe_position_vector,\n",
    "                                burnin_time=burnin_time,\n",
    "                                constant_elongation_rate=fixed_ke,\n",
    "                                fast_output=True)[2]\n",
    "    # Calculating the autocorrelation of the intensity signal\n",
    "    mean_correlation, std_correlation, lags, correlations_array, dwell_time = Correlation(primary_data=ssa_array,\n",
    "                                                                                            max_lag=None, \n",
    "                                                                                            nan_handling='forward_fill',  #forward_fill, 'ignore'\n",
    "                                                                                            shift_data=True,\n",
    "                                                                                            return_full=False,\n",
    "                                                                                            time_interval_between_frames_in_seconds=step_size_in_sec,\n",
    "                                                                                            use_bootstrap=True,\n",
    "                                                                                            show_plot=False,\n",
    "                                                                                            start_lag=0,\n",
    "                                                                                            fit_type='linear',\n",
    "                                                                                            de_correlation_threshold=0.01,\n",
    "                                                                                            correct_baseline=True,\n",
    "                                                                                            use_linear_projection_for_lag_0=False,\n",
    "                                                                                            save_plots=False,\n",
    "                                                                                            use_global_mean= False,\n",
    "                                                                                            remove_outliers = True,\n",
    "                                                                                            #high_outlier_percentile = high_outlier_percentile,\n",
    "                                                                                            #low_outlier_percentile = low_outlier_percentile,\n",
    "                                                                                            plot_individual_trajectories = False,\n",
    "                                                                                            y_axes_min_max_list_values = None, #y_axes_min_max_list_values,\n",
    "                                                                                            x_axes_min_max_list_values=None,\n",
    "                                                                                            multi_tau=True,\n",
    "                                                                                            plot_title=None).run()\n",
    "    \n",
    "    if downsample:\n",
    "        mean_correlation = mean_correlation[::downsample_factor]\n",
    "        std_correlation = std_correlation[::downsample_factor]\n",
    "        lags = lags[::downsample_factor]\n",
    "    \n",
    "\n",
    "    list_mean_correlation_ki.append(mean_correlation)\n",
    "    list_std_correlation_ki.append(std_correlation)\n",
    "    list_lags_ki.append(lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the theoretical G(0) values for each initiation rate.\n",
    "\n",
    "theoretical_decorrelation_fixed_ke = gene_length/fixed_ke\n",
    "\n",
    "list_theoretical_G0 = []\n",
    "for i, ki_tested in enumerate (list_ki):\n",
    "    list_theoretical_G0.append( 1/(ki_tested*theoretical_decorrelation_fixed_ke))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_theoretical_G0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "# Update rcParams to set a white background and Arial fonts.\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',    # figure background is white\n",
    "    'axes.facecolor': 'white',      # axes (plot area) background is white\n",
    "    'axes.edgecolor': 'black',      # axis spines (box) will be black\n",
    "    'axes.linewidth': 1.5,          # thicker border lines for a clear box\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': 'Arial',\n",
    "    'axes.labelsize': 14,           # axis labels: minimum 12\n",
    "    'axes.titlesize': 14,           # title font size: 16\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    # make ticks adn axels labels black fonts\n",
    "    'axes.labelcolor': 'black',\n",
    "    'text.color': 'black',\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'axes.edgecolor': 'black',\n",
    "})\n",
    "for i, mean_correlation in enumerate(list_mean_correlation_ki):\n",
    "    ax.plot(lags, mean_correlation, label='$k_i$='+str(np.round(list_ki[i],2)), linewidth=2)\n",
    "    ax.fill_between(lags, mean_correlation - list_std_correlation_ki[i], mean_correlation + list_std_correlation_ki[i], alpha=0.2)\n",
    "ax.set_xlabel(r'$\\tau (s)$', fontsize=14)\n",
    "\n",
    "ax.set_ylabel(r'$G(\\tau)$', fontsize=14)\n",
    "# legend location top right\n",
    "ax.legend(loc='upper right', fontsize=12)\n",
    "\n",
    "# plot the line of the theoretical decorrelation as a horizontal dashed line of the same color as the plot line.\n",
    "for i, theoretical_G0 in enumerate(list_theoretical_G0):\n",
    "    ax.axhline(y=theoretical_G0, color=ax.get_lines()[i].get_color(), linestyle='--',lw=1)\n",
    "# plot a circle at the theoretical decorrelation value and x =0\n",
    "for i, theoretical_G0 in enumerate(list_theoretical_G0):\n",
    "    ax.plot(0, theoretical_G0, markersize=10, marker='o', color=ax.get_lines()[i].get_color())\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(1.5)\n",
    "ax.set_xlim([-20, 1500])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-tau Algorithm \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Correlation:\n",
    "    \"\"\"\n",
    "    ... [existing docstring content] ...\n",
    "        multi_tau (bool, optional): If True, use multiple-\u03c4 downsampling algorithm for autocorrelation (logarithmic lag spacing). \n",
    "                                    (Note: multi_tau applies **only** to autocorrelation; for cross-correlation this option is ignored.)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        primary_data,\n",
    "        secondary_data=None,\n",
    "        max_lag=None,\n",
    "        nan_handling='zeros',\n",
    "        return_full=True,\n",
    "        use_bootstrap=True,\n",
    "        shift_data=False,\n",
    "        show_plot=False,\n",
    "        save_plots=False,\n",
    "        plot_name='temp_AC.png',\n",
    "        time_interval_between_frames_in_seconds=1,\n",
    "        index_max_lag_for_fit=None,\n",
    "        color_channel=0,\n",
    "        start_lag=0,\n",
    "        line_color='blue',\n",
    "        correct_baseline=False,\n",
    "        baseline_offset=None,\n",
    "        use_global_mean=False,\n",
    "        plot_title=None,\n",
    "        fit_type='linear',\n",
    "        de_correlation_threshold=0.01,\n",
    "        use_linear_projection_for_lag_0=True,\n",
    "        normalize_plot_with_g0=False,\n",
    "        remove_outliers=True,\n",
    "        MAD_THRESHOLD_FACTOR=6.0,\n",
    "        plot_individual_trajectories=False,\n",
    "        y_axes_min_max_list_values=None,\n",
    "        x_axes_min_max_list_values=None,\n",
    "        multi_tau=False    # <-- Added multi_tau parameter\n",
    "    ):\n",
    "        \n",
    "\n",
    "        def shift_and_fill(data1, data2=None, min_nan_threshold=3, fill_with_nans=True):\n",
    "            \"\"\"\n",
    "            Processes two 1D NumPy arrays by removing leading NaNs that exceed a given threshold,\n",
    "            then shifts both arrays left using the shift determined from the first array, and fills\n",
    "            the rightmost part with NaNs or zeros to maintain the original shape.\n",
    "            \"\"\"\n",
    "            if data1.ndim != 1:\n",
    "                raise ValueError(\"Both data1 and data2 must be 1D arrays.\")\n",
    "\n",
    "            nan_count = 0\n",
    "            for value in data1:\n",
    "                if np.isnan(value):\n",
    "                    nan_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            if nan_count >= min_nan_threshold:\n",
    "                fill_value = np.nan if fill_with_nans else 0\n",
    "\n",
    "                new_data1 = np.full_like(data1, fill_value)\n",
    "                new_data1[: len(data1) - nan_count] = data1[nan_count:]\n",
    "\n",
    "                if data2 is not None:\n",
    "                    new_data2 = np.full_like(data2, fill_value)\n",
    "                    new_data2[: len(data2) - nan_count] = data2[nan_count:]\n",
    "                else:\n",
    "                    new_data2 = None\n",
    "                return new_data1, new_data2\n",
    "            return data1, data2\n",
    "        if shift_data:\n",
    "            primary_data_shifted = np.zeros_like(primary_data)\n",
    "            if secondary_data is not None:\n",
    "                secondary_data_shifted = np.zeros_like(secondary_data)\n",
    "            else:\n",
    "                secondary_data_shifted = None\n",
    "\n",
    "            for i in range(primary_data.shape[0]):\n",
    "                if secondary_data is None:\n",
    "                    primary_data_shifted[i, :], _ = shift_and_fill(\n",
    "                        primary_data[i, :], None, min_nan_threshold=2\n",
    "                    )\n",
    "                else:\n",
    "                    primary_data_shifted[i, :], secondary_data_shifted[i, :] = shift_and_fill(\n",
    "                        primary_data[i, :], secondary_data[i, :], min_nan_threshold=2\n",
    "                    )\n",
    "            primary_data = primary_data_shifted\n",
    "            if secondary_data is not None:\n",
    "                secondary_data = secondary_data_shifted\n",
    "\n",
    "        # Store attributes\n",
    "        self.primary_data = primary_data\n",
    "        self.secondary_data = secondary_data\n",
    "        self.max_lag = max_lag\n",
    "        self.nan_handling = nan_handling\n",
    "        self.return_full = return_full\n",
    "        self.use_bootstrap = use_bootstrap\n",
    "        self.BOOTSTRAP_ITERATIONS = 1000\n",
    "        self.time_interval_between_frames_in_seconds = float(time_interval_between_frames_in_seconds)\n",
    "        self.index_max_lag_for_fit = index_max_lag_for_fit\n",
    "        self.plot_name = plot_name\n",
    "        self.save_plots = save_plots\n",
    "        self.show_plot = show_plot\n",
    "        self.color_channel = color_channel\n",
    "        self.start_lag = start_lag\n",
    "        self.line_color = line_color\n",
    "        self.plot_title = plot_title\n",
    "        self.fit_type = fit_type\n",
    "        self.de_correlation_threshold = de_correlation_threshold\n",
    "        self.use_linear_projection_for_lag_0 = use_linear_projection_for_lag_0\n",
    "        self.normalize_plot_with_g0 = normalize_plot_with_g0\n",
    "        self.correct_baseline = correct_baseline\n",
    "        if baseline_offset is None:\n",
    "            # Use half the time series length as default baseline offset\n",
    "            self.baseline_offset = int(primary_data.shape[1] // 2)\n",
    "        else:\n",
    "            self.baseline_offset = baseline_offset\n",
    "        self.use_global_mean = use_global_mean\n",
    "        if correct_baseline:\n",
    "            plot_individual_trajectories = False\n",
    "            print('Baseline correction is enabled. Plotting individual trajectories is disabled due to baseline correction.')\n",
    "        self.remove_outliers = remove_outliers\n",
    "        self.MAD_THRESHOLD_FACTOR = MAD_THRESHOLD_FACTOR\n",
    "        self.plot_individual_trajectories = plot_individual_trajectories\n",
    "        self.y_axes_min_max_list_values = y_axes_min_max_list_values\n",
    "        self.x_axes_min_max_list_values = x_axes_min_max_list_values\n",
    "        self.multi_tau = multi_tau  # <-- Store multi_tau flag\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the correlation calculations (auto or cross) with optional multi-tau downsampling.\n",
    "\n",
    "        Returns:\n",
    "            mean_correlation, error_correlation, lags, correlations_array, dwell_time\n",
    "        \"\"\"\n",
    "        # Determine maximum lag if not set\n",
    "        if self.max_lag is None:\n",
    "            self.max_lag = self.primary_data.shape[1] - 1\n",
    "        else:\n",
    "            if self.max_lag >= self.primary_data.shape[1]:\n",
    "                raise ValueError(\"Max lag cannot be greater than the length of the time series.\")\n",
    "        \n",
    "        # Multi-tau autocorrelation branch\n",
    "        if self.multi_tau and self.secondary_data is None:\n",
    "            # Use multi-tau algorithm for autocorrelation\n",
    "            num_traj = self.primary_data.shape[0]\n",
    "            correlations_list = []\n",
    "            lags_list = []\n",
    "            # Pre-compute global mean if needed\n",
    "            global_mean_val = None\n",
    "            if self.use_global_mean:\n",
    "                global_mean_val = np.nanmean(self.primary_data)\n",
    "            for i in range(num_traj):\n",
    "                # Extract single trajectory and apply NaN trimming and handling\n",
    "                data = self.primary_data[i, :].astype(float)\n",
    "                # Trim leading/trailing NaNs\n",
    "                mask = ~np.isnan(data)\n",
    "                if not np.any(mask):\n",
    "                    # All NaNs, skip this trajectory (no correlation)\n",
    "                    correlations_list.append([]); lags_list.append([])\n",
    "                    continue\n",
    "                start_idx = np.argmax(mask)\n",
    "                end_idx = len(mask) - np.argmax(mask[::-1])\n",
    "                data = data[start_idx:end_idx]\n",
    "                # Handle NaNs according to strategy\n",
    "                if self.nan_handling == \"mean\":\n",
    "                    mean_val = np.nanmean(data) if np.isnan(data).any() else np.mean(data)\n",
    "                    data = np.nan_to_num(data, nan=mean_val)\n",
    "                elif self.nan_handling == \"forward_fill\":\n",
    "                    # Forward fill NaNs within the trimmed segment\n",
    "                    not_nan = ~np.isnan(data)\n",
    "                    if np.any(not_nan):\n",
    "                        first_valid = np.argmax(not_nan)\n",
    "                        last_valid = len(data) - np.argmax(not_nan[::-1]) - 1\n",
    "                        segment = data[first_valid:last_valid + 1]\n",
    "                        mask_seg = np.isnan(segment)\n",
    "                        idx = np.where(~mask_seg, np.arange(len(segment)), 0)\n",
    "                        np.maximum.accumulate(idx, out=idx)\n",
    "                        segment_filled = segment[idx]\n",
    "                        data_filled = np.full_like(data, np.nan)\n",
    "                        data_filled[first_valid:last_valid + 1] = segment_filled\n",
    "                        data = data_filled\n",
    "                elif self.nan_handling == \"ignore\":\n",
    "                    valid_mask = ~np.isnan(data)\n",
    "                    data = data[valid_mask]\n",
    "                elif self.nan_handling == \"zeros\":\n",
    "                    data = np.nan_to_num(data)\n",
    "                if data.size == 0:\n",
    "                    # No valid data after NaN handling\n",
    "                    correlations_list.append([]); lags_list.append([])\n",
    "                    continue\n",
    "\n",
    "                # Subtract mean (global or local)\n",
    "                if self.use_global_mean and global_mean_val is not None:\n",
    "                    local_mean = global_mean_val\n",
    "                else:\n",
    "                    local_mean = np.nanmean(data)\n",
    "                series_orig = data.copy()  # original processed series (with NaNs handled)\n",
    "                cdata = np.nan_to_num(series_orig - local_mean)  # mean-centered series\n",
    "\n",
    "                # Multi-tau correlation computation:\n",
    "                L_current = len(cdata)\n",
    "                factor = 1\n",
    "                level = 0\n",
    "                traj_corr = []\n",
    "                traj_lags = []\n",
    "                # Determine block size m (even integer for multi-tau)\n",
    "                m = 16 if not hasattr(self, 'm') else self.m  # default to 16 if not provided separately\n",
    "                # (Alternatively, m could be made an attribute or parameter if desired)\n",
    "                while True:\n",
    "                    L = L_current\n",
    "                    if L == 0:\n",
    "                        break\n",
    "                    # Define lag range for this level\n",
    "                    if level == 0:\n",
    "                        start_lag = 0\n",
    "                        max_new_lag = min(L - 1, m, self.max_lag)\n",
    "                    else:\n",
    "                        start_lag = 2  # skip 0 and 1 at subsequent levels (avoid overlapping earlier outputs)\n",
    "                        # Max new lag allowed by data length and overall max_lag\n",
    "                        max_new_lag = min(L - 1, m, self.max_lag // factor if factor > 0 else 0)\n",
    "                    if max_new_lag < start_lag:\n",
    "                        break  # no output at this level\n",
    "                    # Compute autocorrelation on the current data (positive lags up to max_new_lag)\n",
    "                    raw_corr = np.correlate(cdata[:L], cdata[:L], mode=\"full\")\n",
    "                    mid_idx = L - 1  # index corresponding to lag 0\n",
    "                    # Loop through requested lags at this level\n",
    "                    for new_k in range(start_lag, max_new_lag + 1):\n",
    "                        orig_lag = new_k * factor\n",
    "                        if orig_lag > self.max_lag:\n",
    "                            break\n",
    "                        overlap = L - new_k  # number of overlapping points for this lag\n",
    "                        # Require a minimum overlap (to avoid very high-noise points)\n",
    "                        min_overlap = max(5, int(0.2 * L))\n",
    "                        if overlap < min_overlap:\n",
    "                            continue  # skip if not enough overlap \n",
    "                        # Compute local normalization using the mean of segments (on original processed series)\n",
    "                        seg1 = series_orig[: L - new_k]\n",
    "                        seg2 = series_orig[new_k: L] \n",
    "                        # (Using current series segments for normalization; they approximate original segments)\n",
    "                        mean1 = np.nanmean(seg1) if seg1.size > 0 else 0.0\n",
    "                        mean2 = np.nanmean(seg2) if seg2.size > 0 else 0.0\n",
    "                        norm_factor = mean1 * mean2\n",
    "                        # Raw correlation value (sum of products) at this lag:\n",
    "                        raw_val = raw_corr[mid_idx + new_k]  \n",
    "                        if norm_factor == 0 or overlap == 0:\n",
    "                            corr_val = np.nan\n",
    "                        else:\n",
    "                            # Normalize by number of pairs (overlap) and mean product\n",
    "                            corr_val = (raw_val / overlap) / norm_factor\n",
    "                        traj_corr.append(corr_val)\n",
    "                        traj_lags.append(orig_lag)\n",
    "                    # Decide whether to continue to next level\n",
    "                    new_length = L // 2  # next downsampled length (drop one if L is odd)\n",
    "                    if new_length < 1 or new_length <= m:\n",
    "                        # Stop if next level would have too few points (<= m) [oai_citation:2\u2021file-padaqanwh6amz255s9e9yh](file://file-PadaQAnWH6aMZ255s9e9yh#:~:text=2%20for%20q%202%20f,it%20will%20assume%20the%20values)\n",
    "                        break\n",
    "                    # Down-sample the series by averaging pairs\n",
    "                    if L % 2 == 1:\n",
    "                        # If odd length, ignore the last point in averaging\n",
    "                        cdata = (cdata[:L-1:2] + cdata[1:L:2]) / 2.0\n",
    "                        series_orig = (series_orig[:L-1:2] + series_orig[1:L:2]) / 2.0\n",
    "                        L_current = new_length  # L//2\n",
    "                    else:\n",
    "                        cdata = (cdata[0:L:2] + cdata[1:L:2]) / 2.0\n",
    "                        series_orig = (series_orig[0:L:2] + series_orig[1:L:2]) / 2.0\n",
    "                        L_current = new_length\n",
    "                    factor *= 2\n",
    "                    level += 1\n",
    "                correlations_list.append(traj_corr)\n",
    "                lags_list.append(traj_lags)\n",
    "            \n",
    "            # Align all trajectories' results to the same lag grid\n",
    "            # Gather unique lag values from all trajectories\n",
    "            all_lags = set()\n",
    "            for lag_arr in lags_list:\n",
    "                all_lags.update(lag_arr)\n",
    "            if len(all_lags) == 0:\n",
    "                # No valid data in any trajectory\n",
    "                mean_correlation = np.full( (1 if self.return_full else 1), np.nan )\n",
    "                error_correlation = np.full_like(mean_correlation, np.nan)\n",
    "                lags = np.array([])  # no lags\n",
    "                correlations_array = np.empty((0, 0))\n",
    "                return mean_correlation, error_correlation, lags, correlations_array, None\n",
    "            all_lags = sorted(all_lags)\n",
    "            # Create correlation_array (N_trajectories x N_lags) and fill with NaN initially\n",
    "            N_traj = len(correlations_list)\n",
    "            N_lags = len(all_lags)\n",
    "            correlations_array = np.full((N_traj, N_lags), np.nan, dtype=float)\n",
    "            for idx, (corr_vals, lag_vals) in enumerate(zip(correlations_list, lags_list)):\n",
    "                if len(corr_vals) == 0:\n",
    "                    continue  # trajectory had no valid correlation\n",
    "                # Map each trajectory's results to the unified lag positions\n",
    "                lag_to_idx = {lag: j for j, lag in enumerate(all_lags)}\n",
    "                for val, lag in zip(corr_vals, lag_vals):\n",
    "                    j = lag_to_idx.get(lag)\n",
    "                    if j is not None:\n",
    "                        correlations_array[idx, j] = val\n",
    "            lags = np.array(all_lags, dtype=float) * self.time_interval_between_frames_in_seconds\n",
    "\n",
    "            # Remove outlier trajectories if enabled\n",
    "            if self.remove_outliers and correlations_array.shape[0] > 0:\n",
    "                traj_means = np.nanmean(correlations_array, axis=1)\n",
    "                median_mean = np.nanmedian(traj_means)\n",
    "                mad = np.nanmedian(np.abs(traj_means - median_mean))\n",
    "                if mad == 0:\n",
    "                    keep_mask = np.ones_like(traj_means, dtype=bool)\n",
    "                else:\n",
    "                    keep_mask = np.abs(traj_means - median_mean) < self.MAD_THRESHOLD_FACTOR * mad\n",
    "                num_removed = np.sum(~keep_mask)\n",
    "                num_total = len(traj_means)\n",
    "                if num_removed > 0:\n",
    "                    print(f\"Warning: Removed {num_removed} outlier trajectories (out of {num_total}) based on a threshold of {self.MAD_THRESHOLD_FACTOR} MAD from the median mean correlation.\")\n",
    "                correlations_array = correlations_array[keep_mask, :]\n",
    "                # Update trajectory count after removal\n",
    "                N_traj = correlations_array.shape[0]\n",
    "\n",
    "            # If no valid trajectories remain:\n",
    "            if correlations_array.size == 0 or N_traj == 0:\n",
    "                mean_correlation = np.full((len(all_lags),), np.nan)\n",
    "                error_correlation = np.full_like(mean_correlation, np.nan)\n",
    "                return mean_correlation, error_correlation, lags, correlations_array, None\n",
    "\n",
    "            # Compute mean correlation across trajectories (ignoring NaNs)\n",
    "            mean_correlation = np.nanmean(correlations_array, axis=0)\n",
    "            # Baseline correction for mean correlation (plateau subtraction) if enabled\n",
    "            if self.correct_baseline:\n",
    "                L_total = len(mean_correlation) - 1  # index of last lag\n",
    "                # Define region for exponential fit: from lag index 2 up to ~99% of max lag\n",
    "                start_idx_fit = 2\n",
    "                end_idx_fit = int(L_total * 0.99) if int(L_total * 0.99) > start_idx_fit else L_total\n",
    "                y_fit = mean_correlation[start_idx_fit: end_idx_fit]\n",
    "                t_fit = np.arange(start_idx_fit, end_idx_fit) * self.time_interval_between_frames_in_seconds\n",
    "                # Initial guesses for exponential fit\n",
    "                if y_fit.size > 0:\n",
    "                    B_guess = y_fit[-1] if not np.isnan(y_fit[-1]) else np.nanmedian(y_fit)\n",
    "                else:\n",
    "                    B_guess = 0.0\n",
    "                A_guess = (mean_correlation[0] - B_guess) if not np.isnan(mean_correlation[0]) else 0.0\n",
    "                tau_guess = (t_fit[-1] - t_fit[0]) / 2.0 if t_fit.size > 1 else 1.0\n",
    "                initial_guess = [A_guess, tau_guess, B_guess]\n",
    "                lower_bounds = [0, 1e-6, np.min(y_fit) if y_fit.size > 0 else 0.0]\n",
    "                upper_bounds = [np.inf, np.inf, mean_correlation[0] if not np.isnan(mean_correlation[0]) else np.inf]\n",
    "                try:\n",
    "                    # Perform exponential decay fit: f(t) = A * exp(-t/tau) + B\n",
    "                    popt, _ = curve_fit(lambda t, A, tau, B: A * np.exp(-t / tau) + B,\n",
    "                                        t_fit[np.isfinite(t_fit) & np.isfinite(y_fit)],\n",
    "                                        y_fit[np.isfinite(t_fit) & np.isfinite(y_fit)],\n",
    "                                        p0=initial_guess, bounds=(lower_bounds, upper_bounds), maxfev=10000)\n",
    "                    _, _, fitted_B = popt\n",
    "                except Exception as e:\n",
    "                    # Fallback: use 10th percentile of tail as baseline estimate\n",
    "                    fitted_B = np.nanpercentile(y_fit, 10) if y_fit.size > 0 else 0.0\n",
    "                # Subtract estimated baseline (plateau) from mean correlation\n",
    "                mean_correlation = mean_correlation - fitted_B\n",
    "\n",
    "            # Compute standard error of the mean via bootstrap or analytic formula\n",
    "            num_kept = correlations_array.shape[0]\n",
    "            if self.use_bootstrap and num_kept > 1:\n",
    "                bootstrap_means = []\n",
    "                rng = np.random.default_rng()\n",
    "                for _ in range(self.BOOTSTRAP_ITERATIONS):\n",
    "                    # Resample trajectories with replacement\n",
    "                    indices = rng.choice(num_kept, size=num_kept, replace=True)\n",
    "                    sample = correlations_array[indices, :]\n",
    "                    m = np.nanmean(sample, axis=0)\n",
    "                    if self.correct_baseline:\n",
    "                        # Subtract baseline from this sample's mean (use tail percentile as baseline)\n",
    "                        offset = max(1, len(m) // 4)  # use last quarter of points for baseline\n",
    "                        baseline_val = np.nanpercentile(m[-offset:], 10)\n",
    "                        m = m - baseline_val\n",
    "                    bootstrap_means.append(m)\n",
    "                bootstrap_means = np.array(bootstrap_means, dtype=float)\n",
    "                error_correlation = np.nanstd(bootstrap_means, axis=0)\n",
    "            else:\n",
    "                # Standard error: std / sqrt(N) (ignoring NaNs)\n",
    "                error_correlation = np.nanstd(correlations_array, axis=0) / np.sqrt(num_kept)\n",
    "            # Lags array is already computed (positive lags only for multi_tau)\n",
    "            # If return_full is False or multi_tau (auto) by design returns only positive lags:\n",
    "            if not self.return_full:\n",
    "                # Already only non-negative lags\n",
    "                pass  # (mean_correlation, error_correlation, etc., are already for lag >= 0)\n",
    "            else:\n",
    "                # For multi_tau autocorrelation, we will **not** mirror negative lags.\n",
    "                # (Auto-correlation is symmetric, but multi-tau returns only positive lags in this implementation.)\n",
    "                pass\n",
    "\n",
    "            # Plotting if requested (autocorrelation plot)\n",
    "            dwell_time = None\n",
    "            if self.show_plot:\n",
    "                dwell_time = mi.Plots().plot_autocorrelation(\n",
    "                    mean_correlation=mean_correlation,\n",
    "                    error_correlation=error_correlation,\n",
    "                    lags=lags,\n",
    "                    correlations_array=correlations_array,\n",
    "                    time_interval_between_frames_in_seconds=self.time_interval_between_frames_in_seconds,\n",
    "                    index_max_lag_for_fit=self.index_max_lag_for_fit,\n",
    "                    start_lag=self.start_lag,\n",
    "                    plot_name=self.plot_name,\n",
    "                    save_plots=self.save_plots,\n",
    "                    line_color=self.line_color,\n",
    "                    plot_title=self.plot_title,\n",
    "                    fit_type=self.fit_type,\n",
    "                    de_correlation_threshold=self.de_correlation_threshold,\n",
    "                    normalize_plot_with_g0=self.normalize_plot_with_g0,\n",
    "                    plot_individual_trajectories=self.plot_individual_trajectories,\n",
    "                    y_axes_min_max_list_values=self.y_axes_min_max_list_values,\n",
    "                    x_axes_min_max_list_values=self.x_axes_min_max_list_values,\n",
    "                )\n",
    "            return mean_correlation, error_correlation, lags, correlations_array, dwell_time\n",
    "\n",
    "        # ... [existing code for standard correlation (no multi_tau or cross-correlation)] ...\n",
    "        # (No changes to the original non-multi_tau branch of run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mean_correlation_ki_multi = []\n",
    "list_std_correlation_ki_multi = []\n",
    "list_lags_ki_multi = []\n",
    "fixed_ke = 5\n",
    "for i, ki_tested in enumerate (list_ki):\n",
    "    ke = calculate_codon_elongation_rates (rna, global_elongation_rate=fixed_ke)\n",
    "    ssa_array = simulate_TASEP_SSA(ki_tested, ke, gene_length, t_max,\n",
    "                                time_interval_in_seconds=step_size_in_sec,\n",
    "                                number_repetitions=number_repetitions, \n",
    "                                first_probe_position_vector=first_probe_position_vector, \n",
    "                                second_probe_position_vector=second_probe_position_vector,\n",
    "                                burnin_time=burnin_time,\n",
    "                                constant_elongation_rate=fixed_ke,\n",
    "                                fast_output=True)[2]\n",
    "    # Calculating the autocorrelation of the intensity signal\n",
    "    mean_correlation_multi, std_correlation_multi, lags_multi, correlations_array_multi, dwell_time_multi = Correlation(primary_data=ssa_array,\n",
    "                                                                                            max_lag=None, \n",
    "                                                                                            nan_handling='forward_fill',  #forward_fill, 'ignore'\n",
    "                                                                                            shift_data=True,\n",
    "                                                                                            return_full=False,\n",
    "                                                                                            time_interval_between_frames_in_seconds=step_size_in_sec,\n",
    "                                                                                            use_bootstrap=True,\n",
    "                                                                                            show_plot=False,\n",
    "                                                                                            start_lag=0,\n",
    "                                                                                            fit_type='linear',\n",
    "                                                                                            de_correlation_threshold=0.01,\n",
    "                                                                                            correct_baseline=True,\n",
    "                                                                                            use_linear_projection_for_lag_0=False,\n",
    "                                                                                            save_plots=False,\n",
    "                                                                                            use_global_mean= False,\n",
    "                                                                                            remove_outliers = True,\n",
    "                                                                                            #high_outlier_percentile = high_outlier_percentile,\n",
    "                                                                                            #low_outlier_percentile = low_outlier_percentile,\n",
    "                                                                                            plot_individual_trajectories = False,\n",
    "                                                                                            y_axes_min_max_list_values = None, #y_axes_min_max_list_values,\n",
    "                                                                                            x_axes_min_max_list_values=None,\n",
    "                                                                                            multi_tau=True,  # Use multi-tau for autocorrelation\n",
    "                                                                                            plot_title=None).run()\n",
    "        \n",
    "    if downsample:\n",
    "        mean_correlation_multi = mean_correlation_multi[::downsample_factor]\n",
    "        std_correlation_multi = std_correlation_multi[::downsample_factor]\n",
    "        lags_multi = lags_multi[::downsample_factor]\n",
    "\n",
    "\n",
    "    list_mean_correlation_ki_multi.append(mean_correlation_multi)\n",
    "    list_std_correlation_ki_multi.append(std_correlation_multi)\n",
    "    list_lags_ki_multi.append(lags_multi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "for i, mean_correlation in enumerate(list_mean_correlation_ki_multi):\n",
    "    ax.plot(lags_multi, mean_correlation, label='$k_i$='+str(np.round(list_ki[i],2)), linewidth=2)\n",
    "    ax.fill_between(lags_multi, mean_correlation - list_std_correlation_ki_multi[i], mean_correlation + list_std_correlation_ki_multi[i], alpha=0.2)\n",
    "ax.set_xlabel(r'$\\tau (s)$', fontsize=14)\n",
    "\n",
    "ax.set_ylabel(r'$G(\\tau)$', fontsize=14)\n",
    "# legend location top right\n",
    "ax.legend(loc='upper right', fontsize=12)\n",
    "\n",
    "# plot the line of the theoretical decorrelation as a horizontal dashed line of the same color as the plot line.\n",
    "for i, theoretical_G0 in enumerate(list_theoretical_G0):\n",
    "    ax.axhline(y=theoretical_G0, color=ax.get_lines()[i].get_color(), linestyle='--',lw=1)\n",
    "# plot a circle at the theoretical decorrelation value and x =0\n",
    "for i, theoretical_G0 in enumerate(list_theoretical_G0):\n",
    "    ax.plot(0, theoretical_G0, markersize=10, marker='o', color=ax.get_lines()[i].get_color())\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(1.5)\n",
    "ax.set_xlim([-20, 1500])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_mean_correlation = []\n",
    "list_std_correlation = []\n",
    "list_lags = []\n",
    "for i, ke_constant in enumerate (list_ke):\n",
    "    ke = calculate_codon_elongation_rates (rna, global_elongation_rate=ke_constant)\n",
    "    ssa_array = simulate_TASEP_SSA(ki, ke, gene_length, t_max,\n",
    "                                time_interval_in_seconds=step_size_in_sec,\n",
    "                                number_repetitions=number_repetitions, \n",
    "                                first_probe_position_vector=first_probe_position_vector, \n",
    "                                second_probe_position_vector=second_probe_position_vector,\n",
    "                                burnin_time=burnin_time,\n",
    "                                constant_elongation_rate=ke_constant,\n",
    "                                fast_output=True)[2]\n",
    "    # Calculating the autocorrelation of the intensity signal\n",
    "    mean_correlation, std_correlation, lags, correlations_array, dwell_time = Correlation(primary_data=ssa_array,\n",
    "                                                                                            max_lag=None,\n",
    "                                                                                            nan_handling='forward_fill',  # forward_fill, 'ignore'\n",
    "                                                                                            shift_data=True,\n",
    "                                                                                            return_full=False,\n",
    "                                                                                            time_interval_between_frames_in_seconds=step_size_in_sec,\n",
    "                                                                                            use_bootstrap=True,\n",
    "                                                                                            show_plot=False,\n",
    "                                                                                            start_lag=0,\n",
    "                                                                                            fit_type='linear',\n",
    "                                                                                            de_correlation_threshold=0.01,\n",
    "                                                                                            correct_baseline=True,\n",
    "                                                                                            use_linear_projection_for_lag_0=True,\n",
    "                                                                                            save_plots=False,\n",
    "                                                                                            use_global_mean= False,\n",
    "                                                                                            remove_outliers = True,\n",
    "                                                                                            MAD_THRESHOLD_FACTOR = MAD_THRESHOLD_FACTOR,\n",
    "                                                                                            #high_outlier_percentile = high_outlier_percentile,\n",
    "                                                                                            #low_outlier_percentile = low_outlier_percentile,\n",
    "                                                                                            plot_individual_trajectories = False,\n",
    "                                                                                            y_axes_min_max_list_values = None, #y_axes_min_max_list_values,\n",
    "                                                                                            x_axes_min_max_list_values=None,\n",
    "                                                                                            multi_tau=True,  # Use standard autocorrelation\n",
    "                                                                                            plot_title=None).run()\n",
    "    \n",
    "    if downsample:\n",
    "        mean_correlation = mean_correlation[::downsample_factor]\n",
    "        std_correlation = std_correlation[::downsample_factor]\n",
    "        lags = lags[::downsample_factor]\n",
    "\n",
    "    list_mean_correlation.append(mean_correlation)\n",
    "    list_std_correlation.append(std_correlation)\n",
    "    list_lags.append(lags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the min_max normalization to list_mean_correlation\n",
    "list_mean_correlation_normalized = list_mean_correlation.copy()\n",
    "for i, correlation in enumerate(list_mean_correlation):\n",
    "    list_mean_correlation_normalized[i] = (correlation - np.nanmin(correlation))/(np.nanmax(correlation) - np.nanmin(correlation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, mean_correlation in enumerate(list_mean_correlation_normalized):\n",
    "    ax.plot(lags, mean_correlation, label='$k_e$='+str(list_ke[i]), linewidth=2)\n",
    "    ax.fill_between(lags, mean_correlation - list_std_correlation[i], mean_correlation + list_std_correlation[i], alpha=0.2)\n",
    "ax.set_xlabel(r'$\\tau (s)$', fontsize=14)\n",
    "ax.set_ylabel(r'$G(\\tau)/G(0)$', fontdict={'fontsize': 14})\n",
    "ax.legend()\n",
    "\n",
    "# plot the line of the theoretical decorrelation as a vertical dashed line of the same color as the plot line. \n",
    "for i, theoretical_decorrelation in enumerate(list_theoretical_decorrelation):\n",
    "    ax.axvline(x=theoretical_decorrelation, color=ax.get_lines()[i].get_color(), linestyle='--',lw=1)\n",
    "# plot a circle at the theoretical decorrelation value and y =0\n",
    "for i, theoretical_decorrelation in enumerate(list_theoretical_decorrelation):\n",
    "    ax.plot(theoretical_decorrelation, 0, markersize=10, marker='o', color=ax.get_lines()[i].get_color())\n",
    "\n",
    "ax.set_xlim([-10, 1500])\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(1.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "def estimate_decorrelation_time_normalized(lags, acf, fit_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Estimate the decorrelation time by normalizing the ACF between 0 and 1,\n",
    "    then performing a linear fit to the early decay portion.\n",
    "    \n",
    "    Parameters:\n",
    "        lags (np.ndarray): Array of lag times.\n",
    "        acf (np.ndarray): Raw autocorrelation function values.\n",
    "        fit_threshold (float): Threshold for the normalized ACF below which\n",
    "                               we stop the fit. (Default: 0.2)\n",
    "                               \n",
    "    Returns:\n",
    "        tau_d (float): Estimated decorrelation time (lag where the fitted line reaches zero).\n",
    "        norm_acf (np.ndarray): Normalized ACF used for fitting.\n",
    "        fit_idx (int): The index up to which the linear fit was performed.\n",
    "    \"\"\"\n",
    "    # Estimate the baseline from the long-lag region (assume last 30% of data)\n",
    "    n = len(acf)\n",
    "    baseline = np.nanmean(acf[int(0.7 * n):])\n",
    "    \n",
    "    # Normalize the ACF so that at lag 0 it is 1 and the plateau becomes 0.\n",
    "    # This assumes acf[0] > baseline.\n",
    "    norm_acf = (acf - baseline) / (acf[0] - baseline)\n",
    "    \n",
    "    # Determine the fitting region: from lag 0 until norm_acf first drops below fit_threshold.\n",
    "    indices = np.where(norm_acf < fit_threshold)[0]\n",
    "    if len(indices) > 0:\n",
    "        fit_idx = indices[0]\n",
    "    else:\n",
    "        # if the ACF never drops below the threshold, use the entire range\n",
    "        fit_idx = n\n",
    "    \n",
    "    # Use the data from lag 0 to fit_idx for the linear regression.\n",
    "    x_fit = lags[:fit_idx]\n",
    "    y_fit = norm_acf[:fit_idx]\n",
    "    \n",
    "    # Perform a linear regression on the fitting region.\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x_fit, y_fit)\n",
    "    \n",
    "    if slope >= 0:\n",
    "        print(\"Warning: The fitted slope is non-negative. Check your ACF or fitting region.\")\n",
    "        return None, norm_acf, fit_idx\n",
    "    \n",
    "    # The fitted line is: y = intercept + slope * x.\n",
    "    # Since we expect the intercept to be close to 1 (at lag 0), \n",
    "    # the decorrelation time is defined as the lag where y=0:\n",
    "    # 0 = intercept + slope * tau_d  ==> tau_d = -intercept / slope.\n",
    "    tau_d = -intercept / slope\n",
    "    return tau_d, norm_acf, fit_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate initiation rate using G(0) = 1/ (ki * tau)\n",
    "calculated_ki = []\n",
    "max_x_range_lags =  150\n",
    "for i, mean_correlation in enumerate(list_mean_correlation_ki):\n",
    "    # Calculate the decorrelation time with threshold method (default threshold=1/e)\n",
    "    tau_estimated = estimate_decorrelation_time_normalized(lags[:max_x_range_lags], mean_correlation[:max_x_range_lags])[0]\n",
    "    print(\"Estimated decorrelation time:\", tau_estimated)\n",
    "    G0 = mean_correlation[0]\n",
    "    ki = 1/(G0*tau_estimated)\n",
    "    calculated_ki.append(ki)\n",
    "print(np.round(calculated_ki,3))\n",
    "print(list_ki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initial conditions\n",
    "ki = 0.057  # Initiation rate\n",
    "global_elongation_rate = 5.33  # Elongation rates for positions 1 to N-1\n",
    "number_repetitions = 100\n",
    "folding_delay = 0 #240\n",
    "added_folding_delay = 0 \n",
    "burnin_time = 800\n",
    "timePerturbationApplication = None #5*60\n",
    "t_max = 360*5 #timePerturbationApplication + 25*60  # Maximum time\n",
    "inhibitor_effectiveness=0.0\n",
    "evaluatingInhibitor = 0\n",
    "\n",
    "\n",
    "time_interval_between_frames_in_seconds = 1 # seconds\n",
    "burnin = 500\n",
    "downsample_time = 5\n",
    "downsample_replicates = 1\n",
    "percentage_to_remove_data = 0  # Remove 80% of the data\n",
    "shift_data = True\n",
    "simulate_photobleacing = True\n",
    "correct_photobleaching = True\n",
    "decay_rate = 0.9  # 1% decrease per minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_delay_to_codons = global_elongation_rate* folding_delay\n",
    "time_delay_to_codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = pathlib.Path('/Users/nzlab-la/Desktop/Advanced_Microscopy/modeling/TASEP/pNZ208_pUB-24xUTagFullLength-KDM5B-MS2.dna')\n",
    "\n",
    "# reading the sequence and extracting the elongation rates\n",
    "protein, rna, dna, indexes_tags, _, seq_record, graphic_features  = read_sequence(seq=file_path, min_protein_length=50,TAG=TAGS)\n",
    "plasmid_figure = plot_plasmid(seq_record, graphic_features,figure_width=25, figure_height=3)\n",
    "\n",
    "gene_length = len(protein)+1 # adding 1 to account for the stop codon\n",
    "tag_positions_first_probe_vector = indexes_tags[0]\n",
    "tag_positions_second_probe_vector = indexes_tags[1] if len(indexes_tags) > 1 else None\n",
    "\n",
    "\n",
    "if folding_delay > 0:\n",
    "    # adding the folding delay as the time when the intensity is generated\n",
    "    time_delay_to_codons = global_elongation_rate* folding_delay\n",
    "    tag_positions_second_probe_vector = [tag_position + time_delay_to_codons for tag_position in tag_positions_second_probe_vector] if tag_positions_second_probe_vector is not None else None\n",
    "    # remove the tags that are not in the gene\n",
    "    tag_positions_second_probe_vector = [tag_position for tag_position in tag_positions_second_probe_vector if tag_position <= gene_length]\n",
    "    if len(tag_positions_second_probe_vector) == 0:\n",
    "        tag_positions_second_probe_vector = None\n",
    "    print(tag_positions_second_probe_vector)\n",
    "\n",
    "first_probe_position_vector = create_probe_vector(tag_positions_first_probe_vector, gene_length)\n",
    "second_probe_position_vector = create_probe_vector(tag_positions_second_probe_vector, gene_length) if tag_positions_second_probe_vector is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path.name.split('.')[0]\n",
    "plasmid_name = file_path.name.split('.')[0].replace('(','_').replace(')','_')\n",
    "plasmid_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ke = calculate_codon_elongation_rates (rna, global_elongation_rate=global_elongation_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pause = False\n",
    "if use_pause:\n",
    "    # adding a pause site by setting the elongation rate to 0.001\n",
    "    ke[-5] = 1/(global_elongation_rate+5) # 1/60 is the elongation rate of the pause site. Meaning this codon takes 60 seconds to be translated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic modeling\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intensity_vector_first_signal_ode,intensity_vector_second_signal_ode = simulate_TASEP_ODE(ki, ke, gene_length, t_max, first_probe_position_vector,second_probe_position_vector,burnin_time)\n",
    "# plt.plot(intensity_vector_first_signal_ode/np.max(intensity_vector_first_signal_ode))\n",
    "# plt.plot(intensity_vector_second_signal_ode/np.max(intensity_vector_second_signal_ode))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling TASEP SSA\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ribosome_trajectories, list_occupancy_output, matrix_intensity_first_signal_RT, matrix_intensity_second_signal_RT = simulate_TASEP_SSA(ki=ki, \n",
    "                                                                                                                                            ke=ke, \n",
    "                                                                                                                                            gene_length=gene_length, \n",
    "                                                                                                                                            t_max=t_max,\n",
    "                                                                                                                                            time_interval_in_seconds=1,\n",
    "                                                                                                                                            number_repetitions=number_repetitions, \n",
    "                                                                                                                                            first_probe_position_vector=first_probe_position_vector,\n",
    "                                                                                                                                            second_probe_position_vector=second_probe_position_vector, \n",
    "                                                                                                                                            constant_elongation_rate=global_elongation_rate,#None,\n",
    "                                                                                                                                            fast_output = True,\n",
    "                                                                                                                                            burnin_time=burnin_time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# improved same version = 30s\n",
    "# improved version 2 = 30 s\n",
    "# rethinkied version = 30 s\n",
    "\n",
    "\n",
    "# rib position 37\n",
    "# old 44 - 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and std of the matrix_intensity_first_signal_RT and matrix_intensity_second_signal_RT\n",
    "mean_first_signal_RT = np.mean(matrix_intensity_first_signal_RT, axis=0)\n",
    "sem_first_signal_RT = np.std(matrix_intensity_first_signal_RT, axis=0)/np.sqrt(number_repetitions)\n",
    "if second_probe_position_vector is not None:\n",
    "    mean_second_signal_RT = np.mean(matrix_intensity_second_signal_RT, axis=0)\n",
    "    sem_second_signal_RT = np.std(matrix_intensity_second_signal_RT, axis=0)/np.sqrt(number_repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a single trajectory for the the two signals\n",
    "plt.figure()\n",
    "selected_trajectory = 0\n",
    "plt.plot(matrix_intensity_first_signal_RT[selected_trajectory,:]/np.max(matrix_intensity_first_signal_RT[selected_trajectory,:]), label='first signal')\n",
    "if second_probe_position_vector is not None:\n",
    "    plt.plot(matrix_intensity_second_signal_RT[selected_trajectory,:]/np.max(matrix_intensity_second_signal_RT[selected_trajectory,:]), label='second signal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the trajectories\n",
    "plt.figure()\n",
    "for i in range(number_repetitions):\n",
    "    plt.plot(matrix_intensity_first_signal_RT[i,:], label='first signal', color='blue', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_correlation_ssa, std_correlation_ssa, lags_ssa, correlations_array_ssa, dwell_time_ssa = mi.Correlation(primary_data=matrix_intensity_first_signal_RT,\n",
    "                                                                                        max_lag=None, \n",
    "                                                                                        nan_handling='forward_fill',  #forward_fill, 'ignore'\n",
    "                                                                                        shift_data=True,\n",
    "                                                                                        return_full=False,\n",
    "                                                                                        time_interval_between_frames_in_seconds=1,\n",
    "                                                                                        use_bootstrap=True,\n",
    "                                                                                        show_plot=True,\n",
    "                                                                                        start_lag=0,\n",
    "                                                                                        fit_type='linear',\n",
    "                                                                                        index_max_lag_for_fit = 300,\n",
    "                                                                                        de_correlation_threshold=0.01,\n",
    "                                                                                        correct_baseline=True,\n",
    "                                                                                        use_linear_projection_for_lag_0=True,\n",
    "                                                                                        save_plots=False,\n",
    "                                                                                        remove_outliers = False,\n",
    "                                                                                        plot_title=None).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_parameters (gene_length, tag_positions_first_probe_vector, g_0, dwell_time):\n",
    "    print ('------------------------------------')\n",
    "    #ke_calculated_ch0 =  np.round( (gene_length-np.max(tag_positions_first_probe_vector)/2) /dwell_time_ch0  , 2)\n",
    "    ke_calculated_ch0 =  np.round( gene_length /dwell_time , 2)\n",
    "    ke_calculated_ch0_corrected =  np.round( (gene_length-(np.max(tag_positions_first_probe_vector))) /dwell_time  , 2)\n",
    "    print ('------------------------------------')\n",
    "    print('Elongation rates: ')\n",
    "    print('Calculated ch0', ke_calculated_ch0 , ' Corrected: ',ke_calculated_ch0_corrected, )\n",
    "    print ('------------------------------------')\n",
    "    print('Initiation rates: ')\n",
    "    # initiation rate\n",
    "    ki_calculated = np.round( 1/ (g_0* dwell_time), 3)\n",
    "    print('Calculated', ki_calculated, ' 1/sec')\n",
    "    print ('------------------------------------')\n",
    "    print('Ribosomal density: ')\n",
    "    # initiation rate\n",
    "    #ribosomal_density = ki_calculated* ke_calculated_ch0_corrected\n",
    "    ribosomal_density = np.round( (gene_length/ke_calculated_ch0_corrected) *ki_calculated , 1)\n",
    "    print('Calculated', np.round(ribosomal_density,1) , ' average number of ribosomes per RNA')\n",
    "    print ('------------------------------------')\n",
    "    print('Ribosomal occurrence: ')\n",
    "    # initiation rate\n",
    "    ribsomal_space = 1/ki_calculated \n",
    "    print('Calculated', np.round(ribsomal_space,2) , ' seconds between ribosome initiation')\n",
    "    print ('------------------------------------')\n",
    "    return None\n",
    "\n",
    "print_parameters(gene_length, tag_positions_first_probe_vector, g_0=0.08, dwell_time=279)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean and std as error shade\n",
    "downsample = 50\n",
    "downsampled_time = np.arange(0,t_max,downsample)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(mean_first_signal_RT,color = 'k',linewidth=2, label='SSA')\n",
    "plt.fill_between(np.arange(len(mean_first_signal_RT)), mean_first_signal_RT-sem_first_signal_RT, mean_first_signal_RT+sem_first_signal_RT, color='k', alpha=0.2)\n",
    "plt.plot(downsampled_time, intensity_vector_first_signal_ode[::downsample], color = 'blue', linestyle='dashed', marker='o', label='ODE')\n",
    "if second_probe_position_vector is not None:\n",
    "    # plot for the second signal\n",
    "    plt.plot(mean_second_signal_RT,color = 'k',linewidth=2, label='SSA')\n",
    "    plt.fill_between(np.arange(len(mean_second_signal_RT)), mean_second_signal_RT-sem_second_signal_RT, mean_second_signal_RT+sem_second_signal_RT, color='k', alpha=0.2)\n",
    "    plt.plot(downsampled_time, intensity_vector_second_signal_ode[::downsample], color = 'red', linestyle='dashed', marker='o', label='ODE')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ribosome movement\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_trajectory = 0\n",
    "\n",
    "#list_ribosome_trajectories, list_occupancy_output, matrix_intensity_first_signa_RT, matrix_intensity_second_signa_RT \n",
    "ribosome_trajectories = list_ribosome_trajectories[selected_trajectory]    \n",
    "ribosome_trajectories = ribosome_trajectories[:,:]\n",
    "intensity_vector_first_signal = matrix_intensity_first_signal_RT[selected_trajectory,:]\n",
    "if second_probe_position_vector is not None:\n",
    "    intensity_vector_second_signal = matrix_intensity_second_signal_RT[selected_trajectory,:]\n",
    "else:\n",
    "    intensity_vector_second_signal = None\n",
    "#plot_RibosomeMovement(ribosome_trajectories, intensity_vector_first_signal ,tag_positions_first_probe_vector,SecondIntensityVector=intensity_vector_second_signal,second_probePositions=tag_positions_second_probe_vector,timePerturbationApplication=timePerturbationApplication) # intensity_vector_second_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ki = str(ki).replace('.','_')\n",
    "str_k = str(global_elongation_rate).replace('.','_')\n",
    "fileNameGif = 'simulation_'+plasmid_name+'_ke_'+str_k+'_ki_'+str_ki + '_inhibitor_effectiveness_'+str(inhibitor_effectiveness)\n",
    "plot_RibosomeMovement_and_Microscope(ribosome_trajectories, intensity_vector_first_signal, tag_positions_first_probe_vector, SecondIntensityVector=intensity_vector_second_signal, second_probePositions=tag_positions_second_probe_vector,FrameVelocity=20,timePerturbationApplication=timePerturbationApplication,fileNameGif=fileNameGif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_positions = tag_positions_first_probe_vector\n",
    "probe_vector = np.zeros(gene_length)\n",
    "for tag in tag_positions:\n",
    "    if tag < gene_length:  # Ensure the tag position is within the gene length\n",
    "        probe_vector[tag:] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Correlations\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(mi)\n",
    "\n",
    "\n",
    "matrix_intensity_first_signal_RT_downsampled = matrix_intensity_first_signal_RT[:,burnin:][::downsample_replicates,::downsample_time]\n",
    "if second_probe_position_vector is not None:\n",
    "    matrix_intensity_second_signal_RT_downsampled = matrix_intensity_second_signal_RT[:,burnin:][::downsample_replicates,::downsample_time]\n",
    "    print('number of replicates : ', matrix_intensity_first_signal_RT_downsampled.shape[0], '\\nnumber of time points : ', matrix_intensity_first_signal_RT_downsampled.shape[1])\n",
    "    mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled, matrix_intensity_second_signal_RT_downsampled)\n",
    "else:\n",
    "    mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if simulate_photobleacing:\n",
    "    decay_rate_first_signal = -np.log(decay_rate) / (100/downsample_time)  # 20% decrease after 100 minutes\n",
    "    decay_rate_second_signal = -np.log(decay_rate) / (100/downsample_time) # 10% decrease after 100 minutes\n",
    "    if second_probe_position_vector is not None:\n",
    "        matrix_intensity_second_signal_RT_downsampled = simulate_photobleaching_in_trajectories(matrix_intensity_second_signal_RT_downsampled, decay_rate_second_signal)\n",
    "        mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled, matrix_intensity_second_signal_RT_downsampled)\n",
    "    else:\n",
    "        matrix_intensity_first_signal_RT_downsampled = simulate_photobleaching_in_trajectories(matrix_intensity_first_signal_RT_downsampled, decay_rate_first_signal)\n",
    "        mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if correct_photobleaching:\n",
    "    decay_rate_first_signal = -np.log(decay_rate) / (100/downsample_time)  # 20% decrease after 100 minutes\n",
    "    decay_rate_second_signal = -np.log(decay_rate) / (100/downsample_time) # 10% decrease after 100 minutes\n",
    "    matrix_intensity_first_signal_RT_downsampled = correct_photobleaching_in_trajectories(matrix_intensity_first_signal_RT_downsampled, decay_rate_first_signal)\n",
    "    if second_probe_position_vector is not None:\n",
    "        matrix_intensity_second_signal_RT_downsampled = correct_photobleaching_in_trajectories(matrix_intensity_second_signal_RT_downsampled, decay_rate_second_signal)\n",
    "        mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled, matrix_intensity_second_signal_RT_downsampled)\n",
    "    else:\n",
    "        mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating misssing data.\n",
    "if second_probe_position_vector is None:\n",
    "    matrix_intensity_first_signal_RT_downsampled,matrix_intensity_second_signal_RT_downsampled = simulate_missing_data(matrix_intensity_first_signal_RT_downsampled, None, percentage_to_remove_data,replace_with='nan')\n",
    "    mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled)\n",
    "else:\n",
    "    matrix_intensity_first_signal_RT_downsampled,matrix_intensity_second_signal_RT_downsampled = simulate_missing_data(matrix_intensity_first_signal_RT_downsampled,matrix_intensity_second_signal_RT_downsampled, percentage_to_remove_data,replace_with='nan')\n",
    "    mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled, matrix_intensity_second_signal_RT_downsampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift the data to the left\n",
    "# importlib.reload(mi)\n",
    "\n",
    "if shift_data == True:\n",
    "    matrix_intensity_first_signal_RT_downsampled  = mi.Utilities().shift_trajectories(matrix_intensity_first_signal_RT_downsampled, )\n",
    "    mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled)\n",
    "    if second_probe_position_vector is not None:\n",
    "        matrix_intensity_first_signal_RT_downsampled, matrix_intensity_second_signal_RT_downsampled = mi.Utilities().shift_trajectories(matrix_intensity_first_signal_RT_downsampled, matrix_intensity_second_signal_RT_downsampled)\n",
    "        mi.Plots().plot_matrix_sample_time(matrix_intensity_first_signal_RT_downsampled, matrix_intensity_second_signal_RT_downsampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(mi)\n",
    "mean_correlation_ch0, std_correlation_ch0, lags_ch0, correlations_array_ch0, dwell_time_ch0 = mi.Correlation(primary_data=matrix_intensity_first_signal_RT_downsampled, max_lag=None, nan_handling='ignore',shift_data=True,return_full=False,time_interval_between_frames_in_seconds=time_interval_between_frames_in_seconds*downsample_time,show_plot=True,start_lag=0,fit_type='linear',de_correlation_threshold=0.05).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi.Plots().plot_autocorrelation( correlations_array_ch0, mean_correlation_ch0,std_correlation_ch0,lags_ch0,5, plot_name='temp_AC.png', save_plots=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if second_probe_position_vector is not None:\n",
    "    mean_correlation_ch1, std_correlation_ch1, lags_ch1, correlations_array_ch1, dwell_time_ch1 = mi.Correlation(primary_data=matrix_intensity_second_signal_RT_downsampled, max_lag=None, nan_handling='ignore',shift_data=True,return_full=False,time_interval_between_frames_in_seconds=time_interval_between_frames_in_seconds*downsample_time,show_plot=True,start_lag=0,fit_type='linear',de_correlation_threshold=0.001).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(mi)\n",
    "if second_probe_position_vector is not None:\n",
    "    mean_cross_correlation, std_cross_correlation, lags_cross_correlation, cross_correlations_array, delay_cross_correlation = mi.Correlation(primary_data=matrix_intensity_first_signal_RT_downsampled, secondary_data=matrix_intensity_second_signal_RT_downsampled, max_lag=None, nan_handling='ignore', shift_data=True, return_full=True,time_interval_between_frames_in_seconds=time_interval_between_frames_in_seconds*downsample_time,show_plot=True).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(lags_cross_correlation,mean_cross_correlation)\n",
    "# # set the xlim between 1000 and 1500\n",
    "# plt.xlim(-500,500)\n",
    "\n",
    "# # plot a vertical line at zero\n",
    "# plt.axvline(x=0, color='k', linestyle='--')\n",
    "# # plot horizontal lines at 0.01\n",
    "# plt.axhline(y=0.01, color='r', linestyle='--')\n",
    "# plt.axhline(y=0.1, color='k', linestyle='--')\n",
    "# plt.axvline(x=-folding_delay, color='k', linestyle='--')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "if second_probe_position_vector is not None:\n",
    "    # calculate the first derivative of the cross correlation\n",
    "    first_derivative = np.diff(mean_cross_correlation)\n",
    "    # smooth the first derivative\n",
    "    #first_derivative = np.convolve(first_derivative, np.ones(20)/20, mode='same')\n",
    "    # plot the first derivative\n",
    "    plt.figure(figsize=(5,3))\n",
    "    plt.plot(lags_cross_correlation[:-1],first_derivative, label='first derivative', color='r', linewidth=4)\n",
    "    plt.xlim(-500,500)\n",
    "\n",
    "    plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "    # add y label as the derivative of the cross correlation with symbols\n",
    "    plt.ylabel('dG/dt')\n",
    "    # x label is tau\n",
    "    plt.xlabel(r'$\\tau$'+ ' (au)')\n",
    "\n",
    "    #plt.axvline(x=folding_delay, color='k', linestyle='--', linewidth=0.5)\n",
    "    plt.axvline(x=-folding_delay, color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('------------------------------------')\n",
    "print('Decorrelation times: ')\n",
    "estimated_decorrelation_time = np.round(  (gene_length-(np.max(tag_positions_first_probe_vector)//2) ) /global_elongation_rate   , 2)\n",
    "print('Estimated decorrelaiton time', estimated_decorrelation_time )\n",
    "\n",
    "\n",
    "#ke_calculated_ch0 =  np.round( (gene_length-np.max(tag_positions_first_probe_vector)/2) /dwell_time_ch0  , 2)\n",
    "ke_calculated_ch0 =  np.round( gene_length /dwell_time_ch0  , 2)\n",
    "ke_calculated_ch0_corrected =  np.round( (gene_length-(np.max(tag_positions_first_probe_vector))) /dwell_time_ch0  , 2)\n",
    "\n",
    "print ('------------------------------------')\n",
    "\n",
    "print('Elongation rates: ')\n",
    "print('Calculated ch0', ke_calculated_ch0 , ' Corrected: ',ke_calculated_ch0_corrected, ', True: ',global_elongation_rate)\n",
    "\n",
    "if second_probe_position_vector is not None:\n",
    "    ke_calculated_ch1 = np.round( ( gene_length /dwell_time_ch1)  , 2)\n",
    "    if second_probe_position_vector is not None:\n",
    "        ke_calculated_ch1_corrected =  np.round( (gene_length-(np.max(tag_positions_second_probe_vector))) /dwell_time_ch1  , 2)\n",
    "        print('Calculated ch1', ke_calculated_ch1, ', True: ',global_elongation_rate)\n",
    "\n",
    "print ('------------------------------------')\n",
    "\n",
    "print('Initiation rates: ')\n",
    "\n",
    "# initiation rate\n",
    "ki_calculated = np.round( 1/ (mean_correlation_ch0[0] * dwell_time_ch0), 3)\n",
    "print('Calculated', ki_calculated, ', True: ',ki)\n",
    "\n",
    "print ('------------------------------------')\n",
    "\n",
    "print('Ribosomal occupancy: ')\n",
    "\n",
    "# make a binary matrix of the ribosome trajectories that are more than 0\n",
    "binary_matrix_ribosomal_occupancy = ribosome_trajectories > 0\n",
    "binary_matrix_ribosomal_occupancy.shape\n",
    "ribosomal_occupancy = np.sum(binary_matrix_ribosomal_occupancy, axis=0)\n",
    "# calculate the ribosomal occupancy\n",
    "theoretical_occupancy = np.round( (gene_length/global_elongation_rate) *ki , 2)\n",
    "print( 'Calculated: ',np.round( np.mean(ribosomal_occupancy) ,2) , ', Theoretical: ',theoretical_occupancy)\n",
    "print ('------------------------------------')\n",
    "\n",
    "if second_probe_position_vector is not None:\n",
    "    print('Folding delay: ')\n",
    "    print('Calculated:' , delay_cross_correlation, ', True: ', folding_delay)\n",
    "print ('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_minima(array, threshold_percentage):\n",
    "    minima_indices = []\n",
    "    for sample_idx, sample in enumerate(array):\n",
    "        # smooth the signal with a moving average of 5 points\n",
    "        sample = np.convolve(sample, np.ones(10)/10, mode='same')\n",
    "        # Normalize the sample between 0 and 100\n",
    "        sample_normalized = (sample - np.min(sample)) / (np.max(sample) - np.min(sample)) * 100\n",
    "        # Define the threshold value\n",
    "        threshold = (threshold_percentage / 100.0) * np.max(sample_normalized)\n",
    "        # Invert the signal to detect minima as peaks\n",
    "        inverted_sample = -sample_normalized\n",
    "        # Use find_peaks to detect peaks in the inverted signal\n",
    "        peaks, properties = find_peaks(inverted_sample,height=-threshold )\n",
    "        # Filter peaks based on the threshold\n",
    "        valid_minima = peaks\n",
    "        minima_indices.append(valid_minima)\n",
    "    return minima_indices\n",
    "\n",
    "def extract_windows(array, indices_list, window_size):\n",
    "    windows = []\n",
    "    for sample_idx, indices in enumerate(indices_list):\n",
    "        sample_windows = []\n",
    "        for idx in indices:\n",
    "            start = idx - window_size\n",
    "            end = idx + window_size + 1\n",
    "            if start >= 0 and end <= array.shape[1]:\n",
    "                window = array[sample_idx, start:end]\n",
    "                sample_windows.append(window)\n",
    "        if sample_windows:\n",
    "            windows.append(np.array(sample_windows))\n",
    "        else:\n",
    "            windows.append(np.array([]))\n",
    "    return windows\n",
    "\n",
    "def calculate_average_profiles(windows):\n",
    "    avg_profiles = []\n",
    "    for sample_windows in windows:\n",
    "        if sample_windows.size == 0:\n",
    "            avg_profiles.append(None)\n",
    "            continue\n",
    "        # Average across all windows at each time point\n",
    "        avg_profile = np.mean(sample_windows, axis=0)\n",
    "        avg_profiles.append(avg_profile)\n",
    "    return avg_profiles\n",
    "\n",
    "def generate_control_indices(array_shape, n_control_points, window_size):\n",
    "    control_indices = []\n",
    "    num_samples = array_shape[0]\n",
    "    num_timepoints = array_shape[1]\n",
    "    min_index = window_size\n",
    "    max_index = num_timepoints - window_size - 1\n",
    "    for sample_idx in range(num_samples):\n",
    "        if max_index < min_index:\n",
    "            control_indices.append(np.array([], dtype=int))\n",
    "            continue\n",
    "        possible_indices = np.arange(min_index, max_index + 1)\n",
    "        n_sample = min(n_control_points, len(possible_indices))\n",
    "        random_indices = np.random.choice(possible_indices, size=n_sample, replace=False)\n",
    "        control_indices.append(random_indices)\n",
    "    return control_indices\n",
    "\n",
    "def plot_results(avg_profiles_1, avg_profiles_2, control_profiles_1, control_profiles_2, window_size):\n",
    "    time_points = np.arange(-window_size, window_size + 1)\n",
    "    # Collect all non-None profiles\n",
    "    valid_avg_profiles_1 = []\n",
    "    valid_avg_profiles_2 = []\n",
    "    valid_control_profiles_1 = []\n",
    "    valid_control_profiles_2 = []\n",
    "    for i in range(len(avg_profiles_1)):\n",
    "        if avg_profiles_1[i] is not None:\n",
    "            valid_avg_profiles_1.append(avg_profiles_1[i])\n",
    "        if avg_profiles_2[i] is not None:\n",
    "            valid_avg_profiles_2.append(avg_profiles_2[i])\n",
    "        if control_profiles_1[i] is not None:\n",
    "            valid_control_profiles_1.append(control_profiles_1[i])\n",
    "        if control_profiles_2[i] is not None:\n",
    "            valid_control_profiles_2.append(control_profiles_2[i])\n",
    "    # Check if there are any valid profiles\n",
    "    if not valid_avg_profiles_1 or not valid_control_profiles_1:\n",
    "        print(\"No valid profiles to plot.\")\n",
    "        return\n",
    "    # Stack the profiles and compute the overall average\n",
    "    overall_avg_profile_1 = np.mean(np.vstack(valid_avg_profiles_1), axis=0)\n",
    "    overall_avg_profile_2 = np.mean(np.vstack(valid_avg_profiles_2), axis=0)\n",
    "    overall_control_profile_1 = np.mean(np.vstack(valid_control_profiles_1), axis=0)\n",
    "    overall_control_profile_2 = np.mean(np.vstack(valid_control_profiles_2), axis=0)\n",
    "    \n",
    "    # Normalize the profiles between 0 and 1\n",
    "    def normalize_profile(profile):\n",
    "        return (profile - np.min(profile)) / (np.max(profile) - np.min(profile))\n",
    "    \n",
    "    #overall_avg_profile_1 = normalize_profile(overall_avg_profile_1)\n",
    "    #overall_avg_profile_2 = normalize_profile(overall_avg_profile_2)\n",
    "    #overall_control_profile_1 = normalize_profile(overall_control_profile_1)\n",
    "    #overall_control_profile_2 = normalize_profile(overall_control_profile_2)\n",
    "    # Plot the overall average profiles\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    #plt.plot(time_points, overall_avg_profile_1, label='Signal 1', color='blue', linewidth=4)\n",
    "    plt.plot(time_points, overall_avg_profile_2, label='Signal 2', color='red', linewidth=4)\n",
    "    #plt.plot(time_points, overall_control_profile_1, label='Control Signal 1', linestyle=':', color='blue', alpha=0.5)\n",
    "    plt.plot(time_points, overall_control_profile_2, label='Control Signal 2', linestyle=':', color='red', alpha=0.5)\n",
    "    # plot a vertical line at time 0\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=folding_delay, color='k', linestyle='--')\n",
    "    # plot a verticla line when the overall_avg_profile_2 is minimum\n",
    "    min_index = np.argmin(overall_avg_profile_2)\n",
    "    plt.axvline(x=time_points[min_index], color='red', linestyle='--', linewidth=1)\n",
    "    print('Intensity surronding minima, Delay: ', time_points[min_index])\n",
    "    plt.title('Intensity Profiles')\n",
    "    plt.xlabel('Time Relative to Minima')\n",
    "    plt.ylabel('Intensity')\n",
    "    # add legend outside the plot to the top right\n",
    "    plt.legend(loc='upper right')\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def analyze_delay(array_1, array_2, threshold_percentage, window_size, n_control_points):\n",
    "    # Detect minima in array_1\n",
    "    minima_indices_1 = detect_minima(array_1, threshold_percentage)\n",
    "    # Extract windows around minima in both arrays\n",
    "    windows_1 = extract_windows(array_1, minima_indices_1, window_size)\n",
    "    windows_2 = extract_windows(array_2, minima_indices_1, window_size)\n",
    "    # Calculate average intensity profiles\n",
    "    avg_profiles_1 = calculate_average_profiles(windows_1)\n",
    "    avg_profiles_2 = calculate_average_profiles(windows_2)\n",
    "    # Generate control data\n",
    "    control_indices = generate_control_indices(array_1.shape, n_control_points, window_size)\n",
    "    # Extract windows around control indices in both arrays\n",
    "    control_windows_1 = extract_windows(array_1, control_indices, window_size)\n",
    "    control_windows_2 = extract_windows(array_2, control_indices, window_size)\n",
    "    # Calculate control average intensity profiles\n",
    "    control_profiles_1 = calculate_average_profiles(control_windows_1)\n",
    "    control_profiles_2 = calculate_average_profiles(control_windows_2)\n",
    "    # Plot the results\n",
    "    plot_results(avg_profiles_1, avg_profiles_2, control_profiles_1, control_profiles_2, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "threshold_percentage = 20  # User-defined threshold percentage\n",
    "window_size = 300            # Number of values before and after the minima\n",
    "n_control_points = 10      # Number of random positions per sample for control data\n",
    "\n",
    "# Run the analysis\n",
    "analyze_delay(matrix_intensity_first_signal_RT, matrix_intensity_second_signal_RT, threshold_percentage, window_size, n_control_points)\n",
    "analyze_delay(matrix_intensity_first_signal_RT_downsampled, matrix_intensity_second_signal_RT_downsampled, threshold_percentage, window_size, n_control_points)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}